# PGO 101 - Chapter 4 ì´ë¡  ê°•ì˜: ë‚˜ë§Œì˜ ì²« Pose Graph Optimizer ì œì‘í•˜ê¸° - ë¹„ì„ í˜• ìµœì í™”ì˜ ì‹¬ì¥ë¶€

**ê°•ì˜ ëª©í‘œ:** ì´ ê°•ì˜ë¥¼ ë§ˆì¹˜ë©´, ì—¬ëŸ¬ë¶„ì€ Pose Graph Optimizationì˜ í•µì‹¬ ì—”ì§„ì´ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ê·¸ ë‚´ë¶€ ì›ë¦¬ë¥¼ ì™„ë²½íˆ ì´í•´í•˜ê²Œ ë©ë‹ˆë‹¤. ë¹„ì„ í˜• ìµœì í™” ë¬¸ì œë¥¼ í‘¸ëŠ” í‘œì¤€ì ì¸ ì ‘ê·¼ë²•ì¸ **Gauss-Newton ì•Œê³ ë¦¬ì¦˜**ê³¼ ê·¸ ê°œì„  ë²„ì „ì¸ **Levenberg-Marquardt ì•Œê³ ë¦¬ì¦˜**ì˜ ìˆ˜í•™ì  ê¸°ì´ˆë¥¼ íƒ„íƒ„íˆ ë‹¤ì§€ê³ , ì´ë¥¼ êµ¬í˜„í•˜ê¸° ìœ„í•´ **Hessian í–‰ë ¬ (H)** ê³¼ **gradient ë²¡í„° (b)** ë¥¼ ì–´ë–»ê²Œ êµ¬ì¶•í•˜ëŠ”ì§€, ê·¸ë¦¬ê³  ì™œ **í¬ì†Œ í–‰ë ¬ (Sparse Matrix)** ì˜ ê°œë…ì´ ëŒ€ê·œëª¨ SLAMì—ì„œ í•„ìˆ˜ì ì¸ì§€ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤. íŠ¹íˆ ì´ˆê¸°ê°’ì´ ë‚˜ì  ë•Œì˜ ìˆ˜ë ´ ë¬¸ì œì™€ ì´ë¥¼ í•´ê²°í•˜ëŠ” ì ì‘ì  ëŒí•‘ ì „ëµê¹Œì§€ ì‹¤ë¬´ì— ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ì§€ì‹ì„ ìŠµë“í•©ë‹ˆë‹¤. ì´ ê°•ì˜ëŠ” `chapter04_building_first_pose_graph_optimizer.ipynb` ì‹¤ìŠµì—ì„œ ì§ì ‘ ì˜µí‹°ë§ˆì´ì €ë¥¼ ì½”ë”©í•˜ê¸° ìœ„í•œ ëª¨ë“  ì´ë¡ ì  ë°°ê²½ì„ ì œê³µí•©ë‹ˆë‹¤.

> ğŸ’¡ **ì´ ì¥ì˜ í•µì‹¬ ì§ˆë¬¸ë“¤:**
> - ì™œ SLAMì„ ë¹„ì„ í˜• ìµœì†Œ ì œê³± ë¬¸ì œë¡œ ê³µì‹í™”í•˜ëŠ”ê°€?
> - Gauss-Newtonì´ Newton ë°©ë²•ë³´ë‹¤ SLAMì— ë” ì í•©í•œ ì´ìœ ëŠ”?
> - Levenberg-Marquardtì˜ ëŒí•‘ íŒŒë¼ë¯¸í„°ê°€ ì–´ë–»ê²Œ ì•ˆì •ì„±ì„ ë³´ì¥í•˜ëŠ”ê°€?
> - í¬ì†Œ í–‰ë ¬ì´ ì—†ë‹¤ë©´ ëŒ€ê·œëª¨ SLAMì´ ë¶ˆê°€ëŠ¥í•œ ì´ìœ ëŠ”?
> - ì´ˆê¸°ê°’ì´ ë‚˜ì  ë•Œ ìµœì í™”ê°€ ì‹¤íŒ¨í•˜ëŠ” ìˆ˜í•™ì  ì›ì¸ì€?

---

## ëª©ì°¨

1. [SLAMì˜ ë¹„ì„ í˜• ìµœì†Œ ì œê³± ê³µì‹í™”](#1-slamì˜-ë¹„ì„ í˜•-ìµœì†Œ-ì œê³±-ê³µì‹í™”)
2. [Newton ë°©ë²•ì—ì„œ Gauss-Newtonìœ¼ë¡œ](#2-newton-ë°©ë²•ì—ì„œ-gauss-newtonìœ¼ë¡œ)
3. [ì •ê·œ ë°©ì •ì‹ê³¼ ì„ í˜• ì‹œìŠ¤í…œ](#3-ì •ê·œ-ë°©ì •ì‹ê³¼-ì„ í˜•-ì‹œìŠ¤í…œ)
4. [Levenberg-Marquardt - ì ì‘ì  ìµœì í™”](#4-levenberg-marquardt---ì ì‘ì -ìµœì í™”)
5. [í¬ì†Œ í–‰ë ¬ì˜ ë§ˆë²•](#5-í¬ì†Œ-í–‰ë ¬ì˜-ë§ˆë²•)
6. [ì„ í˜• ëŒ€ìˆ˜ ì†”ë²„ì˜ ì„ íƒ](#6-ì„ í˜•-ëŒ€ìˆ˜-ì†”ë²„ì˜-ì„ íƒ)
7. [ìˆ˜ì¹˜ì  ì•ˆì •ì„±ê³¼ ì¡°ê±´ìˆ˜](#7-ìˆ˜ì¹˜ì -ì•ˆì •ì„±ê³¼-ì¡°ê±´ìˆ˜)
8. [ì‹¤ì „ êµ¬í˜„ ê³ ë ¤ì‚¬í•­](#8-ì‹¤ì „-êµ¬í˜„-ê³ ë ¤ì‚¬í•­)
9. [ì„±ëŠ¥ ìµœì í™” ì „ëµ](#9-ì„±ëŠ¥-ìµœì í™”-ì „ëµ)
10. [ìš”ì•½ ë° ë‹¤ìŒ ì¥ ì˜ˆê³ ](#10-ìš”ì•½-ë°-ë‹¤ìŒ-ì¥-ì˜ˆê³ )

---

## 1. SLAMì˜ ë¹„ì„ í˜• ìµœì†Œ ì œê³± ê³µì‹í™”

### 1.1 ì™œ ìµœì†Œ ì œê³±ì¸ê°€?

SLAM (Simultaneous Localization and Mapping)ì€ ë³¸ì§ˆì ìœ¼ë¡œ **ë¶ˆí™•ì‹¤ì„± í•˜ì—ì„œì˜ ì¶”ì • ë¬¸ì œ**ì…ë‹ˆë‹¤. ì„¼ì„œëŠ” ë…¸ì´ì¦ˆë¥¼ í¬í•¨í•˜ê³ , ë¡œë´‡ì˜ ì›€ì§ì„ì€ ë¶€ì •í™•í•˜ë©°, í™˜ê²½ì€ ë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ° ìƒí™©ì—ì„œ ìš°ë¦¬ì˜ ëª©í‘œëŠ” ëª¨ë“  ì¸¡ì •ê°’ì„ **ê°€ì¥ ì˜ ì„¤ëª…í•˜ëŠ”** ë¡œë´‡ì˜ ê¶¤ì ê³¼ ì§€ë„ë¥¼ ì°¾ëŠ” ê²ƒì…ë‹ˆë‹¤.

**ë¹„ìš© í•¨ìˆ˜ (Cost Function):**

$$F(\mathbf{x}) = \frac{1}{2} \sum_{(i,j) \in \mathcal{C}} \mathbf{e}_{ij}(\mathbf{x}_i, \mathbf{x}_j)^T \Omega_{ij} \mathbf{e}_{ij}(\mathbf{x}_i, \mathbf{x}_j)$$

ì—¬ê¸°ì„œ:
- $\mathbf{x} = [\mathbf{x}_1, \mathbf{x}_2, ..., \mathbf{x}_n]^T$ : ëª¨ë“  ë¡œë´‡ í¬ì¦ˆì˜ ìƒíƒœ ë²¡í„°
- $\mathbf{e}_{ij}$ : í¬ì¦ˆ $i$ ì™€ $j$ ì‚¬ì´ì˜ **ì”ì°¨ (residual)**
- $\Omega_{ij} = \Sigma_{ij}^{-1}$ : **ì •ë³´ í–‰ë ¬ (Information matrix)**, ì¸¡ì • ë¶ˆí™•ì‹¤ì„±ì˜ ì—­

### 1.2 ì”ì°¨ì˜ ì˜ë¯¸

ì”ì°¨ëŠ” "ì˜ˆì¸¡ê³¼ ì¸¡ì •ì˜ ì°¨ì´"ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤:

$$\mathbf{e}_{ij} = \mathbf{z}_{ij} - \mathbf{h}_{ij}(\mathbf{x}_i, \mathbf{x}_j)$$

- $\mathbf{z}_{ij}$ : ì„¼ì„œê°€ ì¸¡ì •í•œ ìƒëŒ€ ë³€í™˜
- $\mathbf{h}_{ij}$ : í˜„ì¬ í¬ì¦ˆ ì¶”ì •ê°’ìœ¼ë¡œ ì˜ˆì¸¡í•œ ìƒëŒ€ ë³€í™˜

**SE(2) ì˜ˆì‹œ:**
```python
# ì¸¡ì •ê°’: ë¡œë´‡ì´ 1m ì§ì§„, 10ë„ íšŒì „
z_ij = [1.0, 0.0, 0.174]  # [x, y, theta in rad]

# ì˜ˆì¸¡ê°’: í˜„ì¬ í¬ì¦ˆë¡œ ê³„ì‚°
h_ij = inverse(x_i) * x_j  # ìƒëŒ€ ë³€í™˜

# ì”ì°¨
e_ij = z_ij - h_ij
```

### 1.3 ì™œ ë¹„ì„ í˜•ì¸ê°€?

SLAMì˜ ë¹„ì„ í˜•ì„±ì€ ì£¼ë¡œ **íšŒì „ ë³€í™˜**ì—ì„œ ë°œìƒí•©ë‹ˆë‹¤:

1. **íšŒì „ì˜ í•©ì„±**: $R_1 \cdot R_2 \neq R_1 + R_2$
2. **ì‚¼ê°í•¨ìˆ˜**: $\cos(\theta_1 + \theta_2) \neq \cos(\theta_1) + \cos(\theta_2)$
3. **SE(3)ì˜ ë§¤ë‹ˆí´ë“œ êµ¬ì¡°**: ìœ í´ë¦¬ë“œ ê³µê°„ì´ ì•„ë‹Œ ê³¡ë©´

**êµ¬ì²´ì  ì˜ˆì‹œ:**
```python
# ë¹„ì„ í˜• ë³€í™˜
def relative_transform(x_i, x_j):
    # íšŒì „ í–‰ë ¬ (ë¹„ì„ í˜•!)
    R_i = [[cos(x_i[2]), -sin(x_i[2])],
           [sin(x_i[2]),  cos(x_i[2])]]
    
    # ìƒëŒ€ ìœ„ì¹˜ (íšŒì „ì´ ê³±í•´ì ¸ì„œ ë¹„ì„ í˜•)
    t_ij = R_i.T @ (x_j[:2] - x_i[:2])
    
    return t_ij
```

### 1.4 êµ­ì†Œ ìµœì†Œê°’ì˜ í•¨ì •

ë¹„ì„ í˜•ì„±ì˜ ê²°ê³¼ë¡œ ë¹„ìš© í•¨ìˆ˜ëŠ” **ì—¬ëŸ¬ ê°œì˜ ìµœì†Œê°’**ì„ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

<div style="text-align: center;">
<pre>
ë¹„ìš© í•¨ìˆ˜ì˜ ì§€í˜•
     ^
 F(x)|     *     
     |    / \    *
     |   /   \  / \
     |  /     \/   \
     | /            \
     +----------------> x
      êµ­ì†Œ    ì „ì—­
      ìµœì†Œ    ìµœì†Œ
</pre>
</div>

> ğŸ¯ **í•µì‹¬ í†µì°°**: ì¢‹ì€ ì´ˆê¸°ê°’ì´ ì—†ìœ¼ë©´ ì•Œê³ ë¦¬ì¦˜ì´ êµ­ì†Œ ìµœì†Œê°’ì— ê°‡í ìˆ˜ ìˆìŠµë‹ˆë‹¤!

---

## 2. Newton ë°©ë²•ì—ì„œ Gauss-Newtonìœ¼ë¡œ

### 2.1 Newton ë°©ë²•ì˜ ê¸°ë³¸ ì•„ì´ë””ì–´

Newton ë°©ë²•ì€ í•¨ìˆ˜ë¥¼ 2ì°¨ í…Œì¼ëŸ¬ ê¸‰ìˆ˜ë¡œ ê·¼ì‚¬í•©ë‹ˆë‹¤:

$$F(\mathbf{x} + \Delta\mathbf{x}) \approx F(\mathbf{x}) + \mathbf{g}^T \Delta\mathbf{x} + \frac{1}{2} \Delta\mathbf{x}^T \mathbf{H} \Delta\mathbf{x}$$

ì—¬ê¸°ì„œ:
- $\mathbf{g} = \nabla F(\mathbf{x})$ : Gradient
- $\mathbf{H} = \nabla^2 F(\mathbf{x})$ : Hessian

ìµœì†Œê°’ì€ ë¯¸ë¶„ì´ 0ì¸ ì ì—ì„œ ë°œìƒ:
$$\nabla_{\Delta\mathbf{x}} F(\mathbf{x} + \Delta\mathbf{x}) = \mathbf{g} + \mathbf{H} \Delta\mathbf{x} = 0$$

ë”°ë¼ì„œ **Newton ì—…ë°ì´íŠ¸**:
$$\Delta\mathbf{x} = -\mathbf{H}^{-1} \mathbf{g}$$

### 2.2 ìµœì†Œ ì œê³± ë¬¸ì œì˜ íŠ¹ë³„í•œ êµ¬ì¡°

SLAMì˜ ë¹„ìš© í•¨ìˆ˜ëŠ” íŠ¹ë³„í•œ êµ¬ì¡°ë¥¼ ê°€ì§‘ë‹ˆë‹¤:

$$F(\mathbf{x}) = \frac{1}{2} \mathbf{e}(\mathbf{x})^T \Omega \mathbf{e}(\mathbf{x})$$

ì´ë•Œ gradientì™€ Hessianì€:
- **Gradient**: $\mathbf{g} = \mathbf{J}^T \Omega \mathbf{e}$
- **Hessian**: $\mathbf{H} = \mathbf{J}^T \Omega \mathbf{J} + \sum_k \mathbf{e}_k^T \Omega_k \nabla^2 \mathbf{e}_k$

ì—¬ê¸°ì„œ $\mathbf{J} = \frac{\partial \mathbf{e}}{\partial \mathbf{x}}$ ëŠ” ì”ì°¨ì˜ Jacobianì…ë‹ˆë‹¤.

### 2.3 Gauss-Newton ê·¼ì‚¬

**í•µì‹¬ ì•„ì´ë””ì–´**: Hessianì˜ 2ì°¨ í•­ì„ ë¬´ì‹œí•©ë‹ˆë‹¤!

$$\mathbf{H} \approx \mathbf{J}^T \Omega \mathbf{J}$$

**ì™œ ì´ ê·¼ì‚¬ê°€ íƒ€ë‹¹í•œê°€?**

1. **í•´ ê·¼ì²˜ì—ì„œ**: $\mathbf{e} \approx 0$ ì´ë¯€ë¡œ 2ì°¨ í•­ì´ ì‘ìŒ
2. **ê³„ì‚° íš¨ìœ¨ì„±**: 2ì°¨ ë¯¸ë¶„ì„ ê³„ì‚°í•  í•„ìš” ì—†ìŒ
3. **ì–‘ì˜ ì¤€ì •ë¶€í˜¸ì„±**: $\mathbf{J}^T \Omega \mathbf{J}$ ëŠ” í•­ìƒ PSD

### 2.4 Gauss-Newton vs Newton ë¹„êµ

| íŠ¹ì„± | Newton ë°©ë²• | Gauss-Newton |
|------|------------|--------------|
| Hessian ê³„ì‚° | ì™„ì „í•œ 2ì°¨ ë¯¸ë¶„ í•„ìš” | 1ì°¨ ë¯¸ë¶„ë§Œ í•„ìš” |
| ê³„ì‚° ë³µì¡ë„ | $O(n^2)$ Hessian ì›ì†Œ | $O(nm)$ Jacobian ì›ì†Œ |
| ìˆ˜ë ´ ì†ë„ | 2ì°¨ ìˆ˜ë ´ | ê±°ì˜ 2ì°¨ ìˆ˜ë ´ |
| ì•ˆì •ì„± | Hessianì´ ë¹„ì •ë¶€í˜¸ì¼ ìˆ˜ ìˆìŒ | í•­ìƒ í•˜ê°• ë°©í–¥ |
| ë©”ëª¨ë¦¬ | ë” ë§ìŒ | ë” ì ìŒ |

**ìˆ˜ë ´ ë¶„ì„:**

Newton ë°©ë²•ì˜ ìˆ˜ë ´:
$$\|\mathbf{x}_{k+1} - \mathbf{x}^*\| \leq C \|\mathbf{x}_k - \mathbf{x}^*\|^2$$

Gauss-Newtonì˜ ìˆ˜ë ´:
$$\|\mathbf{x}_{k+1} - \mathbf{x}^*\| \leq C_1 \|\mathbf{x}_k - \mathbf{x}^*\|^2 + C_2 \|\mathbf{e}(\mathbf{x}^*)\|$$

> ğŸ’¡ **í•µì‹¬**: Gauss-Newtonì€ ì”ì°¨ê°€ 0ì´ ì•„ë‹ ë•Œ ì•½ê°„ì˜ í¸í–¥ì„ ê°€ì§€ì§€ë§Œ, ì‹¤ìš©ì ìœ¼ë¡œëŠ” ì¶©ë¶„íˆ ë¹ ë¦…ë‹ˆë‹¤!

---

## 3. ì •ê·œ ë°©ì •ì‹ê³¼ ì„ í˜• ì‹œìŠ¤í…œ

### 3.1 ì •ê·œ ë°©ì •ì‹ì˜ ìœ ë„

ê° ë°˜ë³µì—ì„œ ìš°ë¦¬ëŠ” ì„ í˜•í™”ëœ ë¬¸ì œë¥¼ í’‰ë‹ˆë‹¤:

$$\min_{\Delta\mathbf{x}} \|\mathbf{e} + \mathbf{J}\Delta\mathbf{x}\|^2_\Omega$$

ì´ë¥¼ ì „ê°œí•˜ë©´:
$$(\mathbf{e} + \mathbf{J}\Delta\mathbf{x})^T \Omega (\mathbf{e} + \mathbf{J}\Delta\mathbf{x})$$

ë¯¸ë¶„í•˜ì—¬ 0ìœ¼ë¡œ ë†“ìœ¼ë©´:
$$\mathbf{J}^T \Omega \mathbf{J} \Delta\mathbf{x} = -\mathbf{J}^T \Omega \mathbf{e}$$

ì´ê²ƒì´ **ì •ê·œ ë°©ì •ì‹ (Normal Equations)** ì…ë‹ˆë‹¤:
$$\mathbf{H} \Delta\mathbf{x} = -\mathbf{b}$$

### 3.2 ë¸”ë¡ êµ¬ì¡°ì˜ í™œìš©

SLAMì—ì„œ ê° ì œì•½ì€ ëª‡ ê°œì˜ ë³€ìˆ˜ì—ë§Œ ì˜í–¥ì„ ì¤ë‹ˆë‹¤:

```python
# ì—£ì§€ (i,j)ê°€ Hì™€ bì— ê¸°ì—¬í•˜ëŠ” ë¶€ë¶„
def add_edge_contribution(H, b, i, j, e_ij, J_i, J_j, Omega):
    # Hì˜ ë¸”ë¡ ì—…ë°ì´íŠ¸
    H[i,i] += J_i.T @ Omega @ J_i
    H[j,j] += J_j.T @ Omega @ J_j
    H[i,j] += J_i.T @ Omega @ J_j
    H[j,i] += J_j.T @ Omega @ J_i  # ëŒ€ì¹­ì„±
    
    # bì˜ ë¸”ë¡ ì—…ë°ì´íŠ¸
    b[i] += J_i.T @ Omega @ e_ij
    b[j] += J_j.T @ Omega @ e_ij
```

### 3.3 ì •ë³´ í–‰ë ¬ì˜ ì—­í• 

ì •ë³´ í–‰ë ¬ $\Omega = \Sigma^{-1}$ ì€ ì¸¡ì •ì˜ ì‹ ë¢°ë„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤:

**ê³ ì‹ ë¢°ë„ ì¸¡ì • (Odometry):**
$$\Omega_{\text{odom}} = \begin{bmatrix}
100 & 0 & 0 \\
0 & 100 & 0 \\
0 & 0 & 100
\end{bmatrix}$$

**ì €ì‹ ë¢°ë„ ì¸¡ì • (Loop Closure):**
$$\Omega_{\text{loop}} = \begin{bmatrix}
10 & 0 & 0 \\
0 & 10 & 0 \\
0 & 0 & 10
\end{bmatrix}$$

> ğŸ¯ **ì‹¤ìš©ì  íŒ**: ì •ë³´ í–‰ë ¬ì˜ ëŒ€ê° ì›ì†ŒëŠ” ê° ì°¨ì›ì˜ ì •ë°€ë„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. íšŒì „ì´ ìœ„ì¹˜ë³´ë‹¤ ì •í™•í•˜ë‹¤ë©´ íšŒì „ì— ë” í° ê°’ì„ ì„¤ì •í•˜ì„¸ìš”.

---

## 4. Levenberg-Marquardt - ì ì‘ì  ìµœì í™”

### 4.1 Trust Region í•´ì„

Levenberg-MarquardtëŠ” **trust region** ë°©ë²•ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

$$\min_{\Delta\mathbf{x}} \|\mathbf{e} + \mathbf{J}\Delta\mathbf{x}\|^2 \quad \text{subject to} \quad \|\Delta\mathbf{x}\| \leq \delta$$

ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´:
$$(\mathbf{J}^T \Omega \mathbf{J} + \lambda \mathbf{I}) \Delta\mathbf{x} = -\mathbf{J}^T \Omega \mathbf{e}$$

ì—¬ê¸°ì„œ $\lambda$ ëŠ” trust region í¬ê¸°ì™€ ë°˜ë¹„ë¡€í•©ë‹ˆë‹¤.

### 4.2 ëŒí•‘ íŒŒë¼ë¯¸í„°ì˜ ê¸°í•˜í•™ì  ì˜ë¯¸

<div style="text-align: center;">
<pre>
Î»ê°€ ì‘ì„ ë•Œ (Gauss-Newton)     Î»ê°€ í´ ë•Œ (Gradient Descent)
     
    ë“±ê³ ì„                         ë“±ê³ ì„ 
   /     \                      /     \
  |   â€¢â€”â€”â†’|  í° ìŠ¤í…            | â€¢â†’   |  ì‘ì€ ìŠ¤í…
   \     /                      \     /
</pre>
</div>

### 4.3 ì ì‘ì  ëŒí•‘ ì „ëµ

**Marquardtì˜ ì „ëµ:**
```python
def update_lambda(F_new, F_old, lambda_current):
    if F_new < F_old:  # ê°œì„ ë¨
        lambda_new = lambda_current / 10
        accept_step = True
    else:  # ì•…í™”ë¨
        lambda_new = lambda_current * 10
        accept_step = False
    return lambda_new, accept_step
```

**Nielsenì˜ ì „ëµ (gain ratio):**
```python
def nielsen_update(actual_reduction, predicted_reduction, lambda_current):
    rho = actual_reduction / predicted_reduction
    
    if rho > 0.75:
        lambda_new = lambda_current / 3
    elif rho < 0.25:
        lambda_new = lambda_current * 2
    else:
        lambda_new = lambda_current
        
    accept_step = (rho > 0)
    return lambda_new, accept_step
```

### 4.4 LMì˜ ìˆ˜í•™ì  í•´ì„

ëŒí•‘ì´ ì•Œê³ ë¦¬ì¦˜ì„ ì–´ë–»ê²Œ ë³€í™”ì‹œí‚¤ëŠ”ì§€ ë³´ê² ìŠµë‹ˆë‹¤:

**ê³ ìœ ê°’ ë¶„í•´ë¥¼ í†µí•œ ë¶„ì„:**

$\mathbf{H} = \mathbf{V} \Lambda \mathbf{V}^T$ ë¼ê³  í•˜ë©´:

- Gauss-Newton: $\Delta\mathbf{x} = -\mathbf{V} \Lambda^{-1} \mathbf{V}^T \mathbf{b}$
- Levenberg-Marquardt: $\Delta\mathbf{x} = -\mathbf{V} (\Lambda + \lambda \mathbf{I})^{-1} \mathbf{V}^T \mathbf{b}$

ê° ê³ ìœ  ë°©í–¥ì—ì„œ:
$$\Delta x_i = -\frac{v_i^T \mathbf{b}}{\lambda_i + \lambda}$$

- í° ê³ ìœ ê°’ ë°©í–¥ ($\lambda_i \gg \lambda$): ê±°ì˜ ì˜í–¥ ì—†ìŒ
- ì‘ì€ ê³ ìœ ê°’ ë°©í–¥ ($\lambda_i \ll \lambda$): í¬ê²Œ ê°ì‡ ë¨

> ğŸ’¡ **í•µì‹¬ í†µì°°**: LMì€ ë¶ˆí™•ì‹¤í•œ ë°©í–¥(ì‘ì€ ê³ ìœ ê°’)ì˜ ìŠ¤í…ì„ ì œí•œí•©ë‹ˆë‹¤!

---

## 5. í¬ì†Œ í–‰ë ¬ì˜ ë§ˆë²•

### 5.1 ì™œ HëŠ” í¬ì†Œí•œê°€?

SLAM ê·¸ë˜í”„ì˜ ì—°ê²°ì„±ì´ Hì˜ í¬ì†Œì„±ì„ ê²°ì •í•©ë‹ˆë‹¤:

```
í¬ì¦ˆ ê·¸ë˜í”„:          H í–‰ë ¬ êµ¬ì¡°:
                      
1 --- 2               [* * . .]
|     |               [* * * .]
|     |               [. * * *]
4 --- 3               [. . * *]

(ì—°ê²°ëœ í¬ì¦ˆë§Œ Hì—ì„œ ë¹„ì˜ ë¸”ë¡ì„ ë§Œë“¦)
```

**ìˆ˜ì¹˜ì  ì˜ˆì‹œ:**
- 1000ê°œ í¬ì¦ˆ, ê° í¬ì¦ˆë‹¹ í‰ê·  3ê°œ ì—°ê²°
- Dense H: 6000 Ã— 6000 = 36,000,000 ì›ì†Œ
- Sparse H: ~36,000 ë¹„ì˜ ì›ì†Œ (0.1%)
- **ë©”ëª¨ë¦¬ ì ˆì•½**: 1000ë°°!

### 5.2 í¬ì†Œ í–‰ë ¬ ì €ì¥ í˜•ì‹

**COO (Coordinate) í˜•ì‹:**
```python
# (row, col, value) ì‚¼ì¤‘í•­
H_coo = [
    (0, 0, 5.2),
    (0, 1, 1.3),
    (1, 1, 4.1),
    ...
]
```

**CSR (Compressed Sparse Row) í˜•ì‹:**
```python
# í–‰ë³„ë¡œ ì••ì¶• ì €ì¥
values = [5.2, 1.3, 4.1, ...]      # ë¹„ì˜ ê°’ë“¤
col_indices = [0, 1, 1, ...]       # ì—´ ì¸ë±ìŠ¤
row_pointers = [0, 2, 3, ...]      # ê° í–‰ì˜ ì‹œì‘ ìœ„ì¹˜
```

**ì„±ëŠ¥ ë¹„êµ:**

| ì—°ì‚° | Dense | COO | CSR |
|------|-------|-----|-----|
| êµ¬ì¶• | O(nÂ²) | O(nnz) | O(nnz) |
| í–‰ë ¬-ë²¡í„° ê³± | O(nÂ²) | O(nnz) | O(nnz) |
| ì›ì†Œ ì ‘ê·¼ | O(1) | O(nnz) | O(log k) |
| ë©”ëª¨ë¦¬ | nÂ² | 3Ã—nnz | 2Ã—nnz+n |

### 5.3 Fill-in í˜„ìƒê³¼ ìˆœì„œ ë³€ê²½

Cholesky ë¶„í•´ ì¤‘ 0ì´ ë¹„ì˜ì´ ë˜ëŠ” **fill-in** í˜„ìƒ:

```
ì›ë˜ íŒ¨í„´:        ë¶„í•´ í›„:
[* * . .]        [* * â— â—]
[* * * .]   â†’    [* * * â—]
[. * * *]        [â— * * *]
[. . * *]        [â— â— * *]

(â—ëŠ” fill-in)
```

**AMD (Approximate Minimum Degree) ìˆœì„œ ë³€ê²½:**
```python
# ìˆœì„œ ë³€ê²½ìœ¼ë¡œ fill-in ìµœì†Œí™”
perm = amd_ordering(H)
H_reordered = H[perm, :][:, perm]
# ì´ì œ H_reorderedëŠ” ì ì€ fill-inì„ ê°€ì§
```

### 5.4 ì‹¤ì œ SLAMì—ì„œì˜ í¬ì†Œì„± íŒ¨í„´

**ìˆœì°¨ì  SLAM (Odometryë§Œ):**
```
H íŒ¨í„´:
[â–  â–  . . . .]
[â–  â–  â–  . . .]
[. â–  â–  â–  . .]
[. . â–  â–  â–  .]
[. . . â–  â–  â– ]
[. . . . â–  â– ]

(ë  ëŒ€ê° êµ¬ì¡°)
```

**Loop Closureê°€ ìˆëŠ” SLAM:**
```
H íŒ¨í„´:
[â–  â–  . . . â—]
[â–  â–  â–  . . .]
[. â–  â–  â–  . .]
[. . â–  â–  â–  .]
[. . . â–  â–  â– ]
[â— . . . â–  â– ]

(â—ëŠ” loop closureë¡œ ì¸í•œ ì—°ê²°)
```

---

## 6. ì„ í˜• ëŒ€ìˆ˜ ì†”ë²„ì˜ ì„ íƒ

### 6.1 Cholesky ë¶„í•´

ëŒ€ì¹­ ì–‘ì˜ ì •ë¶€í˜¸ í–‰ë ¬ì— ìµœì :

$$\mathbf{H} = \mathbf{L} \mathbf{L}^T$$

**ì¥ì :**
- ê°€ì¥ ë¹ ë¦„ (nÂ³/3 flops)
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì  (Lë§Œ ì €ì¥)
- í¬ì†Œì„± ë³´ì¡´ ê°€ëŠ¥

**ë‹¨ì :**
- Hê°€ ì–‘ì˜ ì •ë¶€í˜¸ì—¬ì•¼ í•¨
- ìˆ˜ì¹˜ì ìœ¼ë¡œ ëœ ì•ˆì •

**êµ¬í˜„:**
```python
from scipy.sparse.linalg import splu
from sksparse.cholmod import cholesky

# Sparse Cholesky
factor = cholesky(H_sparse)
x = factor(b)
```

### 6.2 QR ë¶„í•´

ë” ì•ˆì •ì ì´ì§€ë§Œ ëŠë¦¼:

$$\mathbf{J} = \mathbf{Q} \mathbf{R}$$

ì •ê·œ ë°©ì •ì‹ì„ ê±°ì¹˜ì§€ ì•Šê³  ì§ì ‘:
$$\mathbf{R} \Delta\mathbf{x} = -\mathbf{Q}^T \mathbf{e}$$

**ì¥ì :**
- ìˆ˜ì¹˜ì ìœ¼ë¡œ ë” ì•ˆì •
- ì¡°ê±´ìˆ˜ê°€ âˆšÎº(H) ëŒ€ì‹  Îº(J)
- Rank-deficient ê²½ìš° ì²˜ë¦¬ ê°€ëŠ¥

**ë‹¨ì :**
- ë” ëŠë¦¼ (2nÂ³/3 flops)
- ë” ë§ì€ ë©”ëª¨ë¦¬ í•„ìš”

### 6.3 ë°˜ë³µì  ì†”ë²„

ëŒ€ê·œëª¨ ë¬¸ì œì— ì í•©:

**Conjugate Gradient (CG):**
```python
from scipy.sparse.linalg import cg

# ì „ì²˜ë¦¬ê¸° ì‚¬ìš©
M = create_preconditioner(H)  # e.g., Jacobi, SSOR
x, info = cg(H, b, M=M, tol=1e-6)
```

**ìˆ˜ë ´ ì†ë„:**
$$\|\mathbf{x}_k - \mathbf{x}^*\| \leq 2 \left(\frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1}\right)^k \|\mathbf{x}_0 - \mathbf{x}^*\|$$

### 6.4 ì†”ë²„ ì„ íƒ ê°€ì´ë“œ

| ë¬¸ì œ íŠ¹ì„± | ì¶”ì²œ ì†”ë²„ |
|----------|----------|
| ì‘ì€ dense ë¬¸ì œ | Cholesky |
| ì¤‘ê°„ í¬ê¸°, ì˜ ì¡°ê±´í™”ë¨ | Sparse Cholesky |
| ëŒ€ê·œëª¨, ë§¤ìš° í¬ì†Œ | Conjugate Gradient |
| ì¡°ê±´ìˆ˜ ë‚˜ì¨ | QR ë˜ëŠ” SVD |
| Rank deficient | SVD |
| ì‹¤ì‹œê°„ ìš”êµ¬ì‚¬í•­ | Incremental (iSAM2) |

---

## 7. ìˆ˜ì¹˜ì  ì•ˆì •ì„±ê³¼ ì¡°ê±´ìˆ˜

### 7.1 ì¡°ê±´ìˆ˜ì˜ ì •ì˜ì™€ ì˜ë¯¸

ì¡°ê±´ìˆ˜ëŠ” í–‰ë ¬ì˜ "ë¯¼ê°ë„"ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤:

$$\kappa(\mathbf{H}) = \|\mathbf{H}\| \|\mathbf{H}^{-1}\| = \frac{\lambda_{\max}}{\lambda_{\min}}$$

**ì˜ë¯¸:**
- $\kappa \approx 1$: ì˜ ì¡°ê±´í™”ë¨
- $\kappa \approx 10^6$: ê²½ê³„ì„ 
- $\kappa \approx 10^{12}$: ì‹¬ê°í•˜ê²Œ ill-conditioned

### 7.2 SLAMì—ì„œ ì¡°ê±´ìˆ˜ê°€ ë‚˜ë¹ ì§€ëŠ” ê²½ìš°

1. **ê¸´ ê¶¤ì , Loop closure ì—†ìŒ**
   ```
   ì‹œì‘ â† Â· Â· Â· Â· Â· Â· Â· â†’ ë
   (ë¶ˆí™•ì‹¤ì„±ì´ ëˆ„ì ë¨)
   ```

2. **í‰ë©´ ìš´ë™ë§Œ ìˆëŠ” 3D SLAM**
   - Zì¶• ì •ë³´ ë¶€ì¡±
   - Roll, Pitch ê´€ì¸¡ ë¶ˆê°€

3. **ì„¼ì„œ ì •ë ¬ ë¬¸ì œ**
   - ëª¨ë“  íŠ¹ì§•ì ì´ í•œ ì§ì„ ìƒì—

### 7.3 ì¡°ê±´ìˆ˜ ê°œì„  ì „ëµ

**1. Gauge ì œì•½ ì¶”ê°€:**
```python
# ì²« í¬ì¦ˆ ê³ ì •
H[0:6, 0:6] += 1e10 * np.eye(6)
```

**2. ì „ì²˜ë¦¬ê¸° (Preconditioner):**
```python
# Jacobi ì „ì²˜ë¦¬
D = np.diag(np.diag(H))
H_precond = D^(-1/2) @ H @ D^(-1/2)
```

**3. ì •ê·œí™” (Regularization):**
```python
# Tikhonov ì •ê·œí™”
H_reg = H + alpha * np.eye(n)
```

### 7.4 ìˆ˜ì¹˜ ì •ë°€ë„ ì†ì‹¤ ê°ì§€

```python
def check_numerical_health(H, b, x):
    # ì”ì°¨ ê³„ì‚°
    residual = H @ x - b
    
    # ìƒëŒ€ ì˜¤ì°¨
    relative_error = np.linalg.norm(residual) / np.linalg.norm(b)
    
    # ì¡°ê±´ìˆ˜ ì¶”ì •
    condition = np.linalg.cond(H)
    
    # ì˜ˆìƒ ì •ë°€ë„ ì†ì‹¤
    digits_lost = np.log10(condition)
    
    print(f"ì¡°ê±´ìˆ˜: {condition:.2e}")
    print(f"ì˜ˆìƒ ì •ë°€ë„ ì†ì‹¤: {digits_lost:.1f} ìë¦¬")
```

---

## 8. ì‹¤ì „ êµ¬í˜„ ê³ ë ¤ì‚¬í•­

### 8.1 ë©”ëª¨ë¦¬ ê´€ë¦¬

**í¬ì†Œ í–‰ë ¬ êµ¬ì¶• ìµœì í™”:**
```python
# ë‚˜ìœ ì˜ˆ: ë°˜ë³µì  ì¬í• ë‹¹
H = sparse.csr_matrix((n, n))
for edge in edges:
    H[i, j] += value  # ë§¤ë²ˆ ì¬êµ¬ì¡°í™”!

# ì¢‹ì€ ì˜ˆ: tripletìœ¼ë¡œ ëª¨ì€ í›„ í•œë²ˆì—
rows, cols, values = [], [], []
for edge in edges:
    rows.append(i)
    cols.append(j)
    values.append(value)
H = sparse.coo_matrix((values, (rows, cols))).tocsr()
```

### 8.2 ë³‘ë ¬í™” ì „ëµ

**1. Jacobian ê³„ì‚° ë³‘ë ¬í™”:**
```python
from multiprocessing import Pool

def compute_edge_contribution(edge):
    e, J_i, J_j = compute_residual_and_jacobian(edge)
    return edge.i, edge.j, J_i.T @ Omega @ J_i, J_i.T @ Omega @ e

with Pool() as pool:
    contributions = pool.map(compute_edge_contribution, edges)
```

**2. í–‰ë ¬ ì¡°ë¦½ ë³‘ë ¬í™”:**
```python
# OpenMP ìŠ¤íƒ€ì¼ (C++)
#pragma omp parallel for
for (int k = 0; k < edges.size(); ++k) {
    // ê° ìŠ¤ë ˆë“œê°€ ë…ë¦½ì ì¸ edge ì²˜ë¦¬
    // lock-free accumulation ì‚¬ìš©
}
```

### 8.3 ìˆ˜ë ´ íŒì •

**ë³µí•© ìˆ˜ë ´ ì¡°ê±´:**
```python
def check_convergence(dx, g, F_old, F_new, iteration):
    # 1. Gradient norm
    gradient_converged = np.linalg.norm(g) < 1e-4
    
    # 2. Update norm
    update_converged = np.linalg.norm(dx) < 1e-6
    
    # 3. Relative cost reduction
    relative_reduction = abs(F_old - F_new) / F_old
    cost_converged = relative_reduction < 1e-8
    
    # 4. Maximum iterations
    max_iter_reached = iteration >= max_iterations
    
    return (gradient_converged or update_converged or 
            cost_converged or max_iter_reached)
```

### 8.4 Robust ë¹„ìš© í•¨ìˆ˜

Outlierì— ê°•ê±´í•œ ìµœì í™”:

**Huber ì†ì‹¤:**
$$\rho_\text{Huber}(e) = \begin{cases}
\frac{1}{2}e^2 & \text{if } |e| \leq \delta \\
\delta(|e| - \frac{1}{2}\delta) & \text{otherwise}
\end{cases}$$

**êµ¬í˜„:**
```python
def huber_weight(residual, delta=1.0):
    norm = np.linalg.norm(residual)
    if norm <= delta:
        return 1.0
    else:
        return delta / norm

# ê°€ì¤‘ì¹˜ ì ìš©
w = huber_weight(e)
H += w * J.T @ Omega @ J
b += w * J.T @ Omega @ e
```

---

## 9. ì„±ëŠ¥ ìµœì í™” ì „ëµ

### 9.1 ê³„ì‚° ë³µì¡ë„ ë¶„ì„

| ì—°ì‚° | Dense | Sparse |
|------|-------|---------|
| H êµ¬ì¶• | O(nÂ²m) | O(nm) |
| Cholesky | O(nÂ³) | O(n^{3/2}) |
| Forward/Back solve | O(nÂ²) | O(n) |
| ì´ ë©”ëª¨ë¦¬ | O(nÂ²) | O(n) |

ì—¬ê¸°ì„œ n = ë³€ìˆ˜ ê°œìˆ˜, m = ì œì•½ ê°œìˆ˜

### 9.2 í”„ë¡œíŒŒì¼ë§ê³¼ ë³‘ëª© í˜„ìƒ

```python
import cProfile

def profile_optimization():
    profiler = cProfile.Profile()
    profiler.enable()
    
    # ìµœì í™” ì‹¤í–‰
    optimizer.optimize()
    
    profiler.disable()
    profiler.print_stats(sort='cumulative')
```

**ì „í˜•ì ì¸ ë³‘ëª© ì§€ì :**
1. Jacobian ê³„ì‚° (30-40%)
2. í–‰ë ¬ ì¡°ë¦½ (20-30%)
3. ì„ í˜• ì‹œìŠ¤í…œ í•´ê²° (30-40%)
4. ê¸°íƒ€ (10%)

### 9.3 ìºì‹œ íš¨ìœ¨ì„±

**ë©”ëª¨ë¦¬ ì ‘ê·¼ íŒ¨í„´ ìµœì í™”:**
```python
# ë‚˜ìœ ì˜ˆ: column-major ì ‘ê·¼ in row-major ì €ì¥
for j in range(n):
    for i in range(n):
        process(H[i, j])  # ìºì‹œ ë¯¸ìŠ¤!

# ì¢‹ì€ ì˜ˆ: row-major ì ‘ê·¼
for i in range(n):
    for j in range(n):
        process(H[i, j])  # ìºì‹œ ì¹œí™”ì 
```

### 9.4 Incremental vs Batch

**Batch ìµœì í™”:**
- ëª¨ë“  ë°ì´í„°ë¥¼ í•œë²ˆì— ì²˜ë¦¬
- ìµœì ì˜ ì •í™•ë„
- ê³„ì‚° ë¹„ìš© ë†’ìŒ

**Incremental ìµœì í™” (iSAM2):**
- ìƒˆ ì¸¡ì •ê°’ë§Œ ì²˜ë¦¬
- Bayes tree êµ¬ì¡° í™œìš©
- ì‹¤ì‹œê°„ ê°€ëŠ¥

```python
# Incremental ì—…ë°ì´íŠ¸ ì˜ì‚¬ì½”ë“œ
def incremental_update(new_measurements):
    # 1. ì˜í–¥ë°›ëŠ” ë³€ìˆ˜ ì‹ë³„
    affected = find_affected_variables(new_measurements)
    
    # 2. ë¶€ë¶„ ì„ í˜•í™”
    delta_H, delta_b = linearize_new(new_measurements)
    
    # 3. Bayes tree ì—…ë°ì´íŠ¸
    bayes_tree.update(affected, delta_H, delta_b)
    
    # 4. ë¶€ë¶„ í•´ê²°
    dx = bayes_tree.solve(affected)
```

---

## 10. ìš”ì•½ ë° ë‹¤ìŒ ì¥ ì˜ˆê³ 

### 10.1 í•µì‹¬ ë‚´ìš© ì •ë¦¬

ì´ ì¥ì—ì„œ ë°°ìš´ ë‚´ìš©:

1. **ë¹„ì„ í˜• ìµœì†Œ ì œê³±ì˜ ë³¸ì§ˆ**
   - SLAMì´ ì™œ ì´ í˜•íƒœì¸ì§€
   - ë¹„ì„ í˜•ì„±ì˜ ì›ì¸ê³¼ ê²°ê³¼
   - êµ­ì†Œ ìµœì†Œê°’ì˜ ìœ„í—˜

2. **Gauss-Newtonì˜ ìš°ì•„í•¨**
   - Newton ë°©ë²•ì˜ ë‹¨ìˆœí™”
   - ìµœì†Œ ì œê³± êµ¬ì¡° í™œìš©
   - ê³„ì‚° íš¨ìœ¨ì„±ê³¼ ì•ˆì •ì„±

3. **Levenberg-Marquardtì˜ ì§€í˜œ**
   - Trust region ê°œë…
   - ì ì‘ì  ëŒí•‘ ì „ëµ
   - GNê³¼ GDì˜ ì¥ì  ê²°í•©

4. **í¬ì†Œì„±ì˜ í˜**
   - ê·¸ë˜í”„ êµ¬ì¡°ì™€ í–‰ë ¬ í¬ì†Œì„±
   - 1000ë°° ì„±ëŠ¥ í–¥ìƒ ê°€ëŠ¥
   - Fill-in ìµœì†Œí™” ì „ëµ

5. **ìˆ˜ì¹˜ì  ê°•ê±´ì„±**
   - ì¡°ê±´ìˆ˜ì™€ ì•ˆì •ì„±
   - ì ì ˆí•œ ì†”ë²„ ì„ íƒ
   - ì •ë°€ë„ ì†ì‹¤ ë°©ì§€

### 10.2 ì‹¤ìŠµ ì²´í¬ë¦¬ìŠ¤íŠ¸

`chapter04` ë…¸íŠ¸ë¶ì—ì„œ ë°˜ë“œì‹œ ì‹¤ìŠµí•´ì•¼ í•  ë‚´ìš©:

- [ ] SimplePoseGraphOptimizer í´ë˜ìŠ¤ êµ¬í˜„
- [ ] Residualê³¼ Jacobian ê³„ì‚° êµ¬í˜„
- [ ] H í–‰ë ¬ê³¼ b ë²¡í„° ì¡°ë¦½
- [ ] Gauss-Newton ë°˜ë³µ êµ¬í˜„
- [ ] Levenberg-Marquardt ëŒí•‘ ì¶”ê°€
- [ ] í¬ì†Œ í–‰ë ¬ ì‹œê°í™”ì™€ ë¶„ì„
- [ ] ìˆ˜ë ´ ë¶„ì„ê³¼ ì„±ëŠ¥ ë¹„êµ

### 10.3 ë‹¤ìŒ ì¥ ì˜ˆê³ 

**Chapter 5: ì•¼ì½”ë¹„ì•ˆ - ìˆ˜ë™ ê³„ì‚° vs ìë™ ë¯¸ë¶„**

ë‹¤ìŒ ì¥ì—ì„œëŠ”:
- ë³µì¡í•œ ë³€í™˜ì˜ ì•¼ì½”ë¹„ì•ˆ ìœ ë„
- ìˆ˜ì¹˜ ë¯¸ë¶„ì˜ í•¨ì •
- SymForceë¥¼ ì´ìš©í•œ ìë™ ë¯¸ë¶„
- ì •í™•ì„±ê³¼ íš¨ìœ¨ì„± ë¹„êµ

### 10.4 ì¶”ê°€ í•™ìŠµ ìë£Œ

**ë…¼ë¬¸:**
- Nocedal & Wright, "Numerical Optimization" (2006)
- Dellaert & Kaess, "Factor Graphs for Robot Perception" (2017)
- Kummerle et al., "g2o: A General Framework for Graph Optimization" (2011)

**êµ¬í˜„ ì°¸ê³ :**
- g2o: https://github.com/RainerKuemmerle/g2o
- Ceres Solver: http://ceres-solver.org
- GTSAM: https://gtsam.org

### 10.5 ë§ˆì§€ë§‰ ì¡°ì–¸

> "ìµœì í™”ëŠ” ì˜ˆìˆ ì´ì ê³¼í•™ì…ë‹ˆë‹¤. ìˆ˜í•™ì  ì—„ë°€í•¨ë„ ì¤‘ìš”í•˜ì§€ë§Œ, ì‹¤ì œ ë¬¸ì œì—ì„œëŠ” ì—”ì§€ë‹ˆì–´ë§ ì§ê´€ì´ ë˜‘ê°™ì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì‘ì€ ì˜ˆì œë¶€í„° ì‹œì‘í•´ì„œ ì ì§„ì ìœ¼ë¡œ ë³µì¡ë„ë¥¼ ë†’ì—¬ê°€ì„¸ìš”. ê·¸ë¦¬ê³  í•­ìƒ ì‹œê°í™”í•˜ì„¸ìš” - ìˆ«ìë³´ë‹¤ ê·¸ë¦¼ì´ ë” ë§ì€ ê²ƒì„ ì•Œë ¤ì¤ë‹ˆë‹¤!"

ì´ì œ ì—¬ëŸ¬ë¶„ì€ Pose Graph Optimizerì˜ ì‹¬ì¥ë¶€ë¥¼ ì´í•´í–ˆìŠµë‹ˆë‹¤. ì‹¤ìŠµì„ í†µí•´ ì´ë¡ ì„ ì½”ë“œë¡œ êµ¬í˜„í•˜ë©° ì§„ì •í•œ ì´í•´ë¥¼ ì™„ì„±í•˜ì„¸ìš”!