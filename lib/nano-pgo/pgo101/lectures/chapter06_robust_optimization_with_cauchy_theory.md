# PGO 101 - Chapter 6 이론 강의: 현실의 노이즈와 싸우기 - 강인한 최적화

**강의 목표:** 이 강의를 마치면, 여러분은 실제 SLAM 상황에서 왜 표준 최소 제곱법이 실패할 수 있는지, 그리고 **이상치 (Outlier)** 문제를 해결하기 위한 **강인한 최적화 (Robust Optimization)** 기법이 왜 필수적인지 이해하게 됩니다. 다양한 강인한 비용 함수 (Robust Kernel) 의 원리를 배우고, 이를 **반복적 재가중 최소 제곱법 (IRLS)** 을 통해 구현하는 방법을 설명할 수 있게 됩니다. 이 강의는 `chapter06_robust_optimization_with_cauchy.ipynb` 실습에서 이상치에 강건한 옵티마이저를 구현하기 위한 이론적 기반을 제공합니다.

---

## 1. 표준 최소 제곱법의 함정: 이상치 (Outlier) 문제

지금까지 우리는 비용 함수를 오차의 **제곱**의 합 ($\sum \mathbf{e}^T \Omega \mathbf{e}$) 으로 정의했습니다. 이 방법은 오차 분포가 가우시안 (Gaussian) 정규분포를 따를 때 통계적으로 최적의 해를 제공합니다.

하지만 현실 세계의 센서 데이터는 항상 정규분포를 따르지 않습니다. 때로는 예상치 못한 **아주 큰 오차**, 즉 **이상치 (Outlier)** 가 발생합니다.

*   **SLAM에서의 이상치 발생 원인**:
    *   **잘못된 루프 클로저**: 비슷하게 생긴 다른 장소를 같은 장소로 착각. (가장 치명적!)
    *   **센서 오류**: 햇빛 반사, 일시적인 센서 고장.
    *   **동적 객체**: 움직이는 사람이나 차를 고정된 랜드마크로 오인.
    *   **데이터 연관 실패**: 특징점 (feature) 을 잘못 매칭.

**문제점**: 표준 최소 제곱법은 오차를 **제곱**하기 때문에, 작은 오차는 더 작아지지만 **큰 오차는 훨씬 더 커집니다.**

> 💡 **핵심 비유**: 10명의 반 학생들의 평균 키를 재는데, 한 명의 키를 실수로 200cm가 아닌 2000cm로 입력했다고 상상해보세요. 이 단 하나의 이상치 때문에 반 평균 키는 비현실적으로 커져 버립니다. 표준 최소 제곱법은 이와 같이 이상치에 의해 전체 결과가 크게 왜곡되는 문제에 매우 취약합니다.

## 2. 강인한 비용 함수 (Robust Kernel): 이상치의 영향력 줄이기

이 문제를 해결하기 위해, 오차의 크기에 따라 그 영향력을 동적으로 조절하는 **강인한 비용 함수 (Robust Kernel 또는 Robust Loss Function)** 를 사용합니다.

핵심 아이디어는 간단합니다. **"오차가 일�� 수준을 넘어가면, 그 영향력을 더 이상 제곱으로 증가시키지 않거나, 오히려 줄여버리자!"**

수학적으로, 비용 함수를 다음과 같이 수정합니다.

$$ F(\mathbf{x}) = \sum_{(i,j) \in \mathcal{C}} \rho \left( \sqrt{\mathbf{e}_{ij}^T \Omega_{ij} \mathbf{e}_{ij}} \right) $$

여기서 $\rho(\cdot)$ 가 바로 강인한 비용 함수입니다.

### 주요 강인한 비용 함수

*   **L2 Loss (표준)**: $\rho(e) = \frac{1}{2}e^2$. 강인하지 않음.
*   **Huber Loss**:
    -   **동작**: 작은 오차는 L2처럼, 큰 오차는 L1 (선형 증가) 처럼 동작.
    -   **특징**: 이상치의 영향력을 부드럽게 제한하는 '중재자' 역할.
*   **Cauchy (코시) Loss**:
    -   **동작**: Huber보다 더 공격적으로 큰 오차의 영향력을 줄임.
    -   **특징**: 이상치를 완전히 무시하지는 않지만, 그 영향력을 매우 작게 만드는 '관용적인' 역할.
*   **Tukey Loss**:
    -   **동작**: 특정 임계값을 넘는 오차는 **완전히 무시 (영향력 = 0)**.
    -   **특징**: 가장 공격적으로 이상치를 제거하는 '엄격한 심판' 역할.

### [실습 연결]
`chapter06` 노트북의 **2. Robust Loss Functions** 와 **3. Visualizing Robust Kernels** 섹션에서는, 이 커널들의 수식을 직접 코드��� 구현하고, 각 함수의 모양과 가중치 변화를 시각화하여 그 동작 원리를 직관적으로 비교합니다.

---

## 3. 반복적 재가중 최소 제곱법 (IRLS): 강인한 최적화 구현하기

그렇다면 이 강인한 비용 함수를 어떻게 최적화에 적용할까요? **반복적 재가중 최소 제곱법 (Iteratively Reweighted Least Squares, IRLS)** 이라는 영리한 방법을 사용합니다.

IRLS는 강인한 최적화 문제를, 매 반복마다 **가중치가 달라지는** 표준 최소 제곱 문제로 변환하여 풉니다.

1.  **오차 계산**: 현재 추정치를 기준으로 모든 측정값의 오차 ($\mathbf{e}_{ij}$) 를 계산합니다.
2.  **가중치 계산**: 각 오차의 크기를 강인한 비용 함수 (예: Cauchy 커널) 에 넣어 **가중치 ($w_i$)** 를 계산합니다.
    *   오차가 작으면 -> 가중치가 1에 가까움.
    *   오차가 크면 (이상치) -> 가중치가 0에 가까워짐.
3.  **가중 최소 제곱 문제 풀이**: 이 가중치를 원래의 정보 행렬 ($\Omega$) 에 곱하여 ($\Omega' = w \cdot \Omega$), 가중치가 적용된 새로운 선형 시스템 $H' \Delta \mathbf{x} = -b'$ 를 풉니다.
4.  **상태 업데이트**: 계산된 $\Delta \mathbf{x}$ 로 포즈를 업데이트하고, 수렴할 때까지 이 과정을 반복합니다.

이 방식을 통해, 이상치로 판단되는 측정값은 가중치가 거의 0이 되어 최적화 과정에 거의 영향을 미치지 못하게 됩니다.

### [실습 연결]
`chapter06` 노트북의 **4. Robust Pose Graph Optimizer** 섹션에서는, Chapter 4에서 만든 옵티마이저에 IRLS 로직을 추가하여 이상치에 강건한 옵티마이저를 직접 구현하고, 이상치가 포함된 데이터셋에서 L2와 다른 커널들의 성능을 비교하여 그 효과를 극적으로 확인합니다.