# PGO 101 - Chapter 6 ì´ë¡  ê°•ì˜: í˜„ì‹¤ì˜ ë…¸ì´ì¦ˆì™€ ì‹¸ìš°ê¸° - ê°•ì¸í•œ ìµœì í™”

**ê°•ì˜ ëª©í‘œ:** ì´ ê°•ì˜ë¥¼ ë§ˆì¹˜ë©´, ì—¬ëŸ¬ë¶„ì€ ì‹¤ì œ SLAM ìƒí™©ì—ì„œ ì™œ í‘œì¤€ ìµœì†Œ ì œê³±ë²•ì´ ì‹¤íŒ¨í•  ìˆ˜ ìˆëŠ”ì§€, ê·¸ë¦¬ê³  **ì´ìƒì¹˜ (Outlier)** ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ **ê°•ì¸í•œ ìµœì í™” (Robust Optimization)** ê¸°ë²•ì´ ì™œ í•„ìˆ˜ì ì¸ì§€ ì´í•´í•˜ê²Œ ë©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ê°•ì¸í•œ ë¹„ìš© í•¨ìˆ˜ (Robust Kernel) ì˜ ì›ë¦¬ë¥¼ ë°°ìš°ê³ , ì´ë¥¼ **ë°˜ë³µì  ì¬ê°€ì¤‘ ìµœì†Œ ì œê³±ë²• (IRLS)** ì„ í†µí•´ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤. ì´ ê°•ì˜ëŠ” `chapter06_robust_optimization_with_cauchy.ipynb` ì‹¤ìŠµì—ì„œ ì´ìƒì¹˜ì— ê°•ê±´í•œ ì˜µí‹°ë§ˆì´ì €ë¥¼ êµ¬í˜„í•˜ê¸° ìœ„í•œ ì´ë¡ ì  ê¸°ë°˜ì„ ì œê³µí•©ë‹ˆë‹¤.

> ğŸ’¡ **ì´ ì¥ì˜ í•µì‹¬ ì§ˆë¬¸ë“¤:**
> - ì™œ ì‹¤ì œ SLAMì—ì„œ ì´ìƒì¹˜ê°€ ë¶ˆê°€í”¼í•œê°€?
> - M-estimatorì˜ í†µê³„í•™ì  ê¸°ë°˜ì€ ë¬´ì—‡ì¸ê°€?
> - IRLSê°€ ì–´ë–»ê²Œ ë¹„ë³¼ë¡ ë¬¸ì œë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í‘¸ëŠ”ê°€?
> - ê° robust kernelì˜ í™•ë¥  ë¶„í¬ì  í•´ì„ì€?
> - ìˆ˜ë ´ì„±ì„ ì–´ë–»ê²Œ ë³´ì¥í•  ìˆ˜ ìˆëŠ”ê°€?

---

## ëª©ì°¨

1. [ì´ìƒì¹˜ ë¬¸ì œì˜ ë³¸ì§ˆ](#1-ì´ìƒì¹˜-ë¬¸ì œì˜-ë³¸ì§ˆ)
2. [M-ì¶”ì •ìì˜ í†µê³„í•™ì  ê¸°ì´ˆ](#2-m-ì¶”ì •ìì˜-í†µê³„í•™ì -ê¸°ì´ˆ)
3. [ê°•ì¸í•œ ë¹„ìš© í•¨ìˆ˜ì˜ ìˆ˜í•™ì  ë¶„ì„](#3-ê°•ì¸í•œ-ë¹„ìš©-í•¨ìˆ˜ì˜-ìˆ˜í•™ì -ë¶„ì„)
4. [IRLS ì•Œê³ ë¦¬ì¦˜ì˜ ìœ ë„ì™€ ìˆ˜ë ´ì„±](#4-irls-ì•Œê³ ë¦¬ì¦˜ì˜-ìœ ë„ì™€-ìˆ˜ë ´ì„±)
5. [ì£¼ìš” Robust Kernelì˜ ìƒì„¸ ë¶„ì„](#5-ì£¼ìš”-robust-kernelì˜-ìƒì„¸-ë¶„ì„)
6. [í™•ë¥  ë¶„í¬ì™€ì˜ ì—°ê²°](#6-í™•ë¥ -ë¶„í¬ì™€ì˜-ì—°ê²°)
7. [Chi-squared ê²€ì •ì„ í†µí•œ ì´ìƒì¹˜ íƒì§€](#7-chi-squared-ê²€ì •ì„-í†µí•œ-ì´ìƒì¹˜-íƒì§€)
8. [ê³ ê¸‰ ê¸°ë²•ë“¤](#8-ê³ ê¸‰-ê¸°ë²•ë“¤)
9. [ì‹¤ì „ êµ¬í˜„ ì „ëµ](#9-ì‹¤ì „-êµ¬í˜„-ì „ëµ)
10. [ìš”ì•½ ë° ë‹¤ìŒ ì¥ ì˜ˆê³ ](#10-ìš”ì•½-ë°-ë‹¤ìŒ-ì¥-ì˜ˆê³ )

---

## 1. ì´ìƒì¹˜ ë¬¸ì œì˜ ë³¸ì§ˆ

### 1.1 ìµœëŒ€ ìš°ë„ ì¶”ì •ê³¼ ê°€ìš°ì‹œì•ˆ ê°€ì •

í‘œì¤€ ìµœì†Œ ì œê³±ë²•ì€ **ìµœëŒ€ ìš°ë„ ì¶”ì • (Maximum Likelihood Estimation, MLE)** ì˜ íŠ¹ìˆ˜í•œ ê²½ìš°ì…ë‹ˆë‹¤. ì¸¡ì • ì˜¤ì°¨ê°€ í‰ê·  0, ê³µë¶„ì‚° $\Sigma$ ì¸ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•˜ë©´:

$$p(\mathbf{z} | \mathbf{x}) = \prod_i \frac{1}{\sqrt{(2\pi)^n |\Sigma_i|}} \exp\left(-\frac{1}{2} \mathbf{e}_i^T \Sigma_i^{-1} \mathbf{e}_i\right)$$

ì—¬ê¸°ì„œ $\mathbf{e}_i = \mathbf{z}_i - h_i(\mathbf{x})$ ëŠ” ì”ì°¨ì…ë‹ˆë‹¤. ë¡œê·¸ ìš°ë„ë¥¼ ìµœëŒ€í™”í•˜ë©´:

$$\mathbf{x}^* = \arg\max_{\mathbf{x}} \log p(\mathbf{z} | \mathbf{x}) = \arg\min_{\mathbf{x}} \sum_i \mathbf{e}_i^T \Sigma_i^{-1} \mathbf{e}_i$$

ì´ê²ƒì´ ë°”ë¡œ ìš°ë¦¬ê°€ ìµìˆ™í•œ ìµœì†Œ ì œê³± ë¬¸ì œì…ë‹ˆë‹¤.

### 1.2 ì´ìƒì¹˜ì˜ í†µê³„í•™ì  ëª¨ë¸

ì‹¤ì œ ë°ì´í„°ëŠ” ë‘ ê°€ì§€ ë¶„í¬ì˜ í˜¼í•©ìœ¼ë¡œ ëª¨ë¸ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

$$p(\mathbf{z}) = (1-\epsilon) \cdot p_{\text{inlier}}(\mathbf{z}) + \epsilon \cdot p_{\text{outlier}}(\mathbf{z})$$

- $p_{\text{inlier}}$: ì •ìƒ ì¸¡ì •ê°’ì˜ ë¶„í¬ (ì¢ì€ ê°€ìš°ì‹œì•ˆ)
- $p_{\text{outlier}}$: ì´ìƒì¹˜ì˜ ë¶„í¬ (ë„“ì€ ê°€ìš°ì‹œì•ˆ ë˜ëŠ” ê· ë“± ë¶„í¬)
- $\epsilon$: ì´ìƒì¹˜ ë¹„ìœ¨

### 1.3 SLAMì—ì„œì˜ ì´ìƒì¹˜ ì›ì¸

**1. ì˜ëª»ëœ ë£¨í”„ í´ë¡œì € (Perceptual Aliasing)**
```
ì‹¤ì œ: ë¡œë´‡ì´ ìœ„ì¹˜ Aì—ì„œ Bë¡œ ì´ë™
ì˜ëª»ëœ ì¸ì‹: Bë¥¼ ì´ì „ì— ë°©ë¬¸í•œ Cë¡œ ì°©ê°
ê²°ê³¼: |A-C| >> |A-B| ì¸ ê±°ëŒ€í•œ ì˜¤ì°¨ ë°œìƒ
```

**2. ì„¼ì„œ ì˜¤ë¥˜ì˜ ìœ í˜•**
- **LiDAR**: ìœ ë¦¬ ë°˜ì‚¬, ë¹—ë°©ìš¸, ì•ˆê°œ
- **ì¹´ë©”ë¼**: ëª¨ì…˜ ë¸”ëŸ¬, ì¡°ëª… ë³€í™”, ë Œì¦ˆ í”Œë ˆì–´
- **IMU**: ìê¸°ì¥ ê°„ì„­, ì§„ë™ ì¶©ê²©

**3. ë™ì  í™˜ê²½**
- ì›€ì§ì´ëŠ” ë¬¼ì²´ë¥¼ ì •ì  ëœë“œë§ˆí¬ë¡œ ì˜¤ì¸
- ì‹œê°„ì°¨ ì¸¡ì •ìœ¼ë¡œ ì¸í•œ ë¶ˆì¼ì¹˜

---

## 2. M-ì¶”ì •ìì˜ í†µê³„í•™ì  ê¸°ì´ˆ

### 2.1 M-ì¶”ì •ìì˜ ì •ì˜

**M-ì¶”ì •ì (Maximum likelihood-type estimator)** ëŠ” MLEë¥¼ ì¼ë°˜í™”í•œ ê²ƒìœ¼ë¡œ, ë‹¤ìŒì„ ìµœì†Œí™”í•©ë‹ˆë‹¤:

$$\mathbf{x}^* = \arg\min_{\mathbf{x}} \sum_i \rho(e_i(\mathbf{x}))$$

ì—¬ê¸°ì„œ $\rho(\cdot)$ ëŠ” robust loss functionì…ë‹ˆë‹¤.

### 2.2 ì˜í–¥ í•¨ìˆ˜ (Influence Function)

ì˜í–¥ í•¨ìˆ˜ $\psi(e) = \frac{d\rho(e)}{de}$ ëŠ” í•œ ì¸¡ì •ê°’ì´ ì¶”ì •ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤:

$$\sum_i \psi(e_i) \frac{\partial e_i}{\partial \mathbf{x}} = 0$$

**í•µì‹¬ ì†ì„±:**
- **Bounded**: $|\psi(e)| < M$ for some $M$
- **Redescending**: $\lim_{|e| \to \infty} \psi(e) = 0$

### 2.3 ë¶•ê´´ì  (Breakdown Point)

ë¶•ê´´ì ì€ ì¶”ì •ìê°€ ë¬´ì˜ë¯¸í•´ì§€ê¸° ì „ê¹Œì§€ ê²¬ë”œ ìˆ˜ ìˆëŠ” ìµœëŒ€ ì´ìƒì¹˜ ë¹„ìœ¨ì…ë‹ˆë‹¤:

$$\epsilon^* = \sup\{\epsilon : \sup_{\mathbf{z}_{\text{corrupted}}} ||\hat{\mathbf{x}}(\mathbf{z}_{\text{corrupted}}) - \mathbf{x}_{\text{true}}|| < \infty\}$$

**ì˜ˆì‹œ:**
- í‰ê· : $\epsilon^* = 0$ (ë‹¨ í•˜ë‚˜ì˜ ì´ìƒì¹˜ë„ ì¹˜ëª…ì )
- ì¤‘ì•™ê°’: $\epsilon^* = 0.5$ (50%ê¹Œì§€ ê²¬ë”¤)
- Huber M-estimator: $\epsilon^* = 0$ (í•˜ì§€ë§Œ ì˜í–¥ì€ ì œí•œë¨)

---

## 3. ê°•ì¸í•œ ë¹„ìš© í•¨ìˆ˜ì˜ ìˆ˜í•™ì  ë¶„ì„

### 3.1 ë³¼ë¡ì„±ê³¼ ìµœì í™”

ë¹„ìš© í•¨ìˆ˜ì˜ ë³¼ë¡ì„±ì€ ìµœì í™”ì˜ ìˆ˜ë ´ì„±ì„ ê²°ì •í•©ë‹ˆë‹¤:

**ì •ì˜:** $\rho(e)$ ê°€ ë³¼ë¡í•˜ë ¤ë©´:
$$\rho(\lambda e_1 + (1-\lambda) e_2) \leq \lambda \rho(e_1) + (1-\lambda) \rho(e_2)$$

ëª¨ë“  $e_1, e_2$ ì™€ $\lambda \in [0,1]$ ì— ëŒ€í•´ ì„±ë¦½í•´ì•¼ í•©ë‹ˆë‹¤.

### 3.2 ì ê·¼ì  íš¨ìœ¨ì„± (Asymptotic Efficiency)

ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ í•˜ì—ì„œ M-ì¶”ì •ìì˜ ìƒëŒ€ì  íš¨ìœ¨ì„±:

$$\text{ARE} = \frac{\text{Var}[\hat{\mathbf{x}}_{\text{MLE}}]}{\text{Var}[\hat{\mathbf{x}}_{\text{M-estimator}}]}$$

**Trade-off**: ì´ìƒì¹˜ì— ê°•ê±´í• ìˆ˜ë¡ ì •ìƒ ë°ì´í„°ì—ì„œì˜ íš¨ìœ¨ì„±ì€ ê°ì†Œí•©ë‹ˆë‹¤.

---

## 4. IRLS ì•Œê³ ë¦¬ì¦˜ì˜ ìœ ë„ì™€ ìˆ˜ë ´ì„±

### 4.1 ìˆ˜í•™ì  ìœ ë„

ê°•ê±´í•œ ë¹„ìš© í•¨ìˆ˜ ìµœì†Œí™” ë¬¸ì œ:

$$\min_{\mathbf{x}} F(\mathbf{x}) = \sum_i \rho(e_i(\mathbf{x}))$$

ìµœì ì„± ì¡°ê±´ (1ì°¨ í•„ìš”ì¡°ê±´):

$$\nabla F = \sum_i \psi(e_i) \nabla e_i = 0$$

ì—¬ê¸°ì„œ $\psi(e) = \rho'(e)$ ëŠ” ì˜í–¥ í•¨ìˆ˜ì…ë‹ˆë‹¤.

### 4.2 ê°€ì¤‘ì¹˜ í•¨ìˆ˜ ë„ì…

$w(e) = \frac{\psi(e)}{e}$ for $e \neq 0$ ë¡œ ì •ì˜í•˜ë©´:

$$\sum_i w(e_i) e_i \nabla e_i = 0$$

ì´ëŠ” ê°€ì¤‘ ìµœì†Œ ì œê³± ë¬¸ì œì˜ ì •ê·œ ë°©ì •ì‹ê³¼ ë™ì¼í•©ë‹ˆë‹¤!

### 4.3 IRLS ì•Œê³ ë¦¬ì¦˜

**ë°˜ë³µ $k$ ì—ì„œ:**

1. **ì”ì°¨ ê³„ì‚°**: $e_i^{(k)} = z_i - h_i(\mathbf{x}^{(k)})$

2. **ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸**: 
   $$w_i^{(k)} = \frac{\psi(||e_i^{(k)}||)}{||e_i^{(k)}||}$$

3. **ì„ í˜•í™”**: $e_i(\mathbf{x}) \approx e_i^{(k)} + J_i^{(k)} \Delta\mathbf{x}$

4. **ê°€ì¤‘ ì •ê·œ ë°©ì •ì‹ í’€ì´**:
   $$\left(\sum_i J_i^T W_i^{(k)} J_i\right) \Delta\mathbf{x} = -\sum_i J_i^T W_i^{(k)} e_i^{(k)}$$
   
   ì—¬ê¸°ì„œ $W_i^{(k)} = w_i^{(k)} \Omega_i$

5. **ì—…ë°ì´íŠ¸**: $\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + \Delta\mathbf{x}$

### 4.4 ìˆ˜ë ´ì„± ì¦ëª… (ë³¼ë¡ ì»¤ë„ì˜ ê²½ìš°)

**ì •ë¦¬:** $\rho(e)$ ê°€ ë³¼ë¡í•˜ê³  ë¯¸ë¶„ê°€ëŠ¥í•˜ë©´, IRLSëŠ” ë‹¨ì¡° ê°ì†Œ ì„±ì§ˆì„ ë§Œì¡±í•©ë‹ˆë‹¤:

$$F(\mathbf{x}^{(k+1)}) \leq F(\mathbf{x}^{(k)})$$

**ì¦ëª… ìŠ¤ì¼€ì¹˜:**

1. Taylor ì „ê°œ:
   $$\rho(e^{(k+1)}) \approx \rho(e^{(k)}) + \psi(e^{(k)})(e^{(k+1)} - e^{(k)})$$

2. ê°€ì¤‘ ìµœì†Œ ì œê³± í•´ëŠ” ë‹¤ìŒì„ ìµœì†Œí™”:
   $$\sum_i w_i^{(k)} (e_i^{(k+1)})^2$$

3. ë³¼ë¡ì„±ê³¼ Jensen ë¶€ë“±ì‹ì„ ì´ìš©í•˜ì—¬ ê°ì†Œ ì„±ì§ˆ ì¦ëª…

---

## 5. ì£¼ìš” Robust Kernelì˜ ìƒì„¸ ë¶„ì„

### 5.1 L2 (Gaussian) Kernel

$$\rho(e) = \frac{1}{2}e^2, \quad \psi(e) = e, \quad w(e) = 1$$

**íŠ¹ì„±:**
- ë³¼ë¡ì„±: âœ“ (ê°•ë³¼ë¡)
- ì˜í–¥ í•¨ìˆ˜: ë¬´í•œëŒ€ (unbounded)
- ìµœì ì„±: ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆì— ìµœì 

### 5.2 Huber Kernel

$$\rho(e) = \begin{cases}
\frac{1}{2}e^2 & |e| \leq k \\
k|e| - \frac{1}{2}k^2 & |e| > k
\end{cases}$$

$$\psi(e) = \begin{cases}
e & |e| \leq k \\
k \cdot \text{sign}(e) & |e| > k
\end{cases}$$

$$w(e) = \begin{cases}
1 & |e| \leq k \\
k/|e| & |e| > k
\end{cases}$$

**íŠ¹ì„±:**
- ë³¼ë¡ì„±: âœ“
- ì˜í–¥ í•¨ìˆ˜: ì œí•œë¨ (bounded)
- ë§¤ê°œë³€ìˆ˜ ì„ íƒ: $k = 1.345\sigma$ ëŠ” 95% íš¨ìœ¨ì„± ì œê³µ

### 5.3 Cauchy (Lorentzian) Kernel

$$\rho(e) = \frac{c^2}{2} \log\left(1 + \left(\frac{e}{c}\right)^2\right)$$

$$\psi(e) = \frac{e}{1 + (e/c)^2}$$

$$w(e) = \frac{1}{1 + (e/c)^2}$$

**íŠ¹ì„±:**
- ë³¼ë¡ì„±: âœ— (ë¹„ë³¼ë¡)
- ì˜í–¥ í•¨ìˆ˜: Redescending
- ì ê·¼ì  í–‰ë™: $\rho(e) \sim c^2 \log|e|$ as $|e| \to \infty$

### 5.4 Tukey Biweight Kernel

$$\rho(e) = \begin{cases}
\frac{c^2}{6}\left[1 - \left(1 - \left(\frac{e}{c}\right)^2\right)^3\right] & |e| \leq c \\
\frac{c^2}{6} & |e| > c
\end{cases}$$

$$\psi(e) = \begin{cases}
e\left(1 - \left(\frac{e}{c}\right)^2\right)^2 & |e| \leq c \\
0 & |e| > c
\end{cases}$$

**íŠ¹ì„±:**
- ë³¼ë¡ì„±: âœ— (ë¹„ë³¼ë¡)
- ì˜í–¥ í•¨ìˆ˜: Hard redescending
- ì™„ì „ ê±°ë¶€: $|e| > c$ ì¸ ì¸¡ì •ê°’ì€ ë¬´ì‹œ

### 5.5 ë¹„êµ ë¶„ì„

```
ì—ëŸ¬ í¬ê¸°ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ ë³€í™”:
e/c    | L2   | Huber | Cauchy | Tukey
-------|------|-------|--------|-------
0.5    | 1.00 | 1.00  | 0.80   | 0.94
1.0    | 1.00 | 1.00  | 0.50   | 0.00
2.0    | 1.00 | 0.50  | 0.20   | 0.00
5.0    | 1.00 | 0.20  | 0.04   | 0.00
```

---

## 6. í™•ë¥  ë¶„í¬ì™€ì˜ ì—°ê²°

### 6.1 M-ì¶”ì •ìì™€ í™•ë¥  ë¶„í¬

ê° robust kernelì€ íŠ¹ì • ì˜¤ì°¨ ë¶„í¬ì— ëŒ€í•œ MLEë¡œ í•´ì„ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

$$\rho(e) = -\log p(e) + \text{const}$$

### 6.2 Cauchy ë¶„í¬

Cauchy (Lorentzian) ë¶„í¬ì˜ í™•ë¥  ë°€ë„ í•¨ìˆ˜:

$$p(e; \gamma) = \frac{1}{\pi\gamma \left(1 + \left(\frac{e}{\gamma}\right)^2\right)}$$

ì´ëŠ” Cauchy kernelê³¼ ì§ì ‘ ì—°ê²°ë©ë‹ˆë‹¤:

$$-\log p(e) \propto \log\left(1 + \left(\frac{e}{\gamma}\right)^2\right) = \frac{2}{c^2}\rho_{\text{Cauchy}}(e)$$

**íŠ¹ì§•:**
- Heavy-tailed ë¶„í¬ (ê¼¬ë¦¬ê°€ $1/e^2$ ë¡œ ê°ì†Œ)
- í‰ê· ê³¼ ë¶„ì‚°ì´ ì •ì˜ë˜ì§€ ì•ŠìŒ
- ì´ìƒì¹˜ì— ë§¤ìš° ê°•ê±´

### 6.3 Student's t-ë¶„í¬

ììœ ë„ $\nu$ ì¸ t-ë¶„í¬:

$$p(e; \nu, \sigma) = \frac{\Gamma\left(\frac{\nu+1}{2}\right)}{\Gamma\left(\frac{\nu}{2}\right)\sqrt{\nu\pi\sigma^2}} \left(1 + \frac{e^2}{\nu\sigma^2}\right)^{-\frac{\nu+1}{2}}$$

- $\nu \to \infty$: ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¡œ ìˆ˜ë ´
- $\nu = 1$: Cauchy ë¶„í¬
- $\nu = 4-6$: ì‹¤ìš©ì ì¸ ì„ íƒ

---

## 7. Chi-squared ê²€ì •ì„ í†µí•œ ì´ìƒì¹˜ íƒì§€

### 7.1 ì •ê·œí™”ëœ ì”ì°¨

ìµœì í™” í›„ ì”ì°¨ì˜ í†µê³„ì  ê²€ì •:

$$r_i = \frac{e_i}{\sqrt{\sigma_i^2}} = \frac{e_i}{\sqrt{(J_i \Sigma_x J_i^T + R_i)_{ii}}}$$

ì—¬ê¸°ì„œ:
- $\Sigma_x = (J^T \Omega J)^{-1}$: ìƒíƒœ ì¶”ì •ì˜ ê³µë¶„ì‚°
- $R_i$: ì¸¡ì • ë…¸ì´ì¦ˆ ê³µë¶„ì‚°

### 7.2 Chi-squared ê²€ì •

ê°€ìš°ì‹œì•ˆ ê°€ì • í•˜ì—ì„œ $r_i^2 \sim \chi^2(m)$, ì—¬ê¸°ì„œ $m$ ì€ ì¸¡ì •ê°’ì˜ ì°¨ì›ì…ë‹ˆë‹¤.

**ì´ìƒì¹˜ íŒë³„:**
$$r_i^2 > \chi^2_{m,1-\alpha} \Rightarrow \text{ì´ìƒì¹˜ë¡œ íŒë³„}$$

ì—¬ê¸°ì„œ $\alpha$ ëŠ” ìœ ì˜ìˆ˜ì¤€ (ì˜ˆ: 0.05)

### 7.3 Mahalanobis ê±°ë¦¬

ë‹¤ë³€ëŸ‰ ì¸¡ì •ê°’ì˜ ê²½ìš°:

$$d_i^2 = \mathbf{e}_i^T \Sigma_i^{-1} \mathbf{e}_i$$

ì´ëŠ” $\chi^2(m)$ ë¶„í¬ë¥¼ ë”°ë¥´ë©°, ë‹¤ì°¨ì› ì´ìƒì¹˜ ê²€ì •ì— ì‚¬ìš©ë©ë‹ˆë‹¤.

---

## 8. ê³ ê¸‰ ê¸°ë²•ë“¤

### 8.1 Graduated Non-Convexity (GNC)

ë¹„ë³¼ë¡ ë¬¸ì œë¥¼ ì ì§„ì ìœ¼ë¡œ í•´ê²°í•˜ëŠ” ì „ëµ:

**ì•Œê³ ë¦¬ì¦˜:**
1. ì´ˆê¸°: ë³¼ë¡ ê·¼ì‚¬ (ì˜ˆ: Huber with large $k$)
2. ë°˜ë³µì ìœ¼ë¡œ ë¹„ë³¼ë¡ì„± ì¦ê°€:
   $$\rho_\mu(e) = (1-\mu)\rho_{\text{convex}}(e) + \mu\rho_{\text{non-convex}}(e)$$
3. $\mu: 0 \to 1$ ë¡œ ì ì§„ì  ì¦ê°€

**ìˆ˜ë ´ì„± ê°œì„ :**
- ë‚˜ìœ êµ­ì†Œ ìµœì†Ÿê°’ íšŒí”¼
- ì´ˆê¸°ê°’ ë¯¼ê°ë„ ê°ì†Œ

### 8.2 Switchable Constraints

ì´ì§„ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•œ ëª…ì‹œì  ì´ìƒì¹˜ ê±°ë¶€:

$$\min_{\mathbf{x}, \mathbf{s}} \sum_i s_i \rho(e_i(\mathbf{x})) + \lambda \sum_i (1-s_i)$$

ì—¬ê¸°ì„œ $s_i \in \{0,1\}$ ëŠ” ì¸¡ì •ê°’ $i$ ì˜ í™œì„±í™” ì—¬ë¶€

**ì™„í™” ê¸°ë²•:**
- ì—°ì† ì™„í™”: $s_i \in [0,1]$
- êµëŒ€ ìµœì í™”: $\mathbf{x}$ ì™€ $\mathbf{s}$ ë¥¼ ë²ˆê°ˆì•„ ìµœì í™”

### 8.3 Dynamic Covariance Scaling (DCS)

ì ì‘ì  ì •ë³´ í–‰ë ¬ ì¡°ì •:

$$\Omega_i^{(k)} = \gamma_i^{(k)} \Omega_i^{(0)}$$

ì—¬ê¸°ì„œ ìŠ¤ì¼€ì¼ íŒ©í„°:
$$\gamma_i^{(k)} = \frac{\psi(||e_i^{(k)}||)}{||e_i^{(k)}||}$$

### 8.4 Maximum Correntropy Criterion (MCC)

ì •ë³´ ì´ë¡ ì  ì ‘ê·¼:

$$\max_{\mathbf{x}} \sum_i \exp\left(-\frac{e_i(\mathbf{x})^2}{2\sigma^2}\right)$$

ì´ëŠ” Gaussian kernelì„ ì‚¬ìš©í•œ correntropy ìµœëŒ€í™”ì™€ ë™ì¼í•©ë‹ˆë‹¤.

---

## 9. ì‹¤ì „ êµ¬í˜„ ì „ëµ

### 9.1 ì»¤ë„ ë§¤ê°œë³€ìˆ˜ ì„ íƒ

**1. MAD (Median Absolute Deviation) ê¸°ë°˜:**
$$\hat{\sigma} = 1.4826 \times \text{MAD} = 1.4826 \times \text{median}(|e_i - \text{median}(e_i)|)$$

**2. ì„¼ì„œë³„ íŠœë‹:**
```python
# LiDAR: ì‘ì€ threshold (ì •ë°€ ì„¼ì„œ)
cauchy_lidar = CauchyKernel(delta=0.1)  # 10cm

# ì¹´ë©”ë¼: ì¤‘ê°„ threshold
cauchy_camera = CauchyKernel(delta=2.0)  # 2 pixels

# íœ  ì˜¤ë„ë©”íŠ¸ë¦¬: í° threshold (ìŠ¬ë¦½ ê³ ë ¤)
cauchy_wheel = CauchyKernel(delta=0.05)  # 5% of distance
```

### 9.2 ë‹¤ë‹¨ê³„ ìµœì í™” ì „ëµ

```python
def multi_stage_optimization(optimizer):
    # Stage 1: Conservative (Huber)
    optimizer.kernel = HuberKernel(delta=3.0*sigma)
    optimizer.optimize(max_iter=10)
    
    # Stage 2: Moderate (Cauchy)
    optimizer.kernel = CauchyKernel(delta=1.5*sigma)
    optimizer.optimize(max_iter=10)
    
    # Stage 3: Aggressive (Tukey)
    optimizer.kernel = TukeyKernel(delta=3.0*sigma)
    optimizer.optimize(max_iter=5)
```

### 9.3 ìˆ˜ë ´ ê°€ì† ê¸°ë²•

**1. Damping (Levenberg-Marquardt):**
$$(J^T W J + \lambda I) \Delta\mathbf{x} = -J^T W \mathbf{e}$$

**2. Line Search:**
$$\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + \alpha^* \Delta\mathbf{x}$$

ì—¬ê¸°ì„œ $\alpha^* = \arg\min_\alpha F(\mathbf{x}^{(k)} + \alpha \Delta\mathbf{x})$

### 9.4 ì‹¤ì‹œê°„ SLAMì„ ìœ„í•œ ìµœì í™”

**1. Incremental ì—…ë°ì´íŠ¸:**
- ìƒˆ ì¸¡ì •ê°’ë§Œ ì²˜ë¦¬
- ì´ì „ ê°€ì¤‘ì¹˜ ì¬ì‚¬ìš©

**2. ë³‘ë ¬í™”:**
```python
# ì”ì°¨ì™€ ê°€ì¤‘ì¹˜ ê³„ì‚° ë³‘ë ¬í™”
parallel_for(edges) {
    compute_error_and_weight(edge)
}

# í¬ì†Œ í–‰ë ¬ ì¡°ë¦½ ë³‘ë ¬í™”
parallel_reduce(H_matrix, b_vector)
```

### 9.5 í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•

```python
class HybridRobustOptimizer:
    def optimize(self):
        # 1. RANSACìœ¼ë¡œ ëª…ë°±í•œ ì´ìƒì¹˜ ì œê±°
        inliers = ransac_filter(measurements)
        
        # 2. Robust optimizationìœ¼ë¡œ ì •ì œ
        result = irls_optimize(inliers, CauchyKernel())
        
        # 3. Chi-squared testë¡œ ì¶”ê°€ ì´ìƒì¹˜ ê²€ì¶œ
        final_inliers = chi_squared_filter(result)
        
        # 4. ìµœì¢… ìµœì í™”
        return optimize(final_inliers)
```

---

## 10. ìš”ì•½ ë° ë‹¤ìŒ ì¥ ì˜ˆê³ 

### 10.1 í•µì‹¬ ê°œë… ì •ë¦¬

1. **ì´ìƒì¹˜ì˜ ë¶ˆê°€í”¼ì„±**: ì‹¤ì œ SLAMì—ì„œ ì„¼ì„œ ì˜¤ë¥˜, ë™ì  í™˜ê²½, ë°ì´í„° ì—°ê´€ ì‹¤íŒ¨ë¡œ ì¸í•œ ì´ìƒì¹˜ëŠ” í”¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

2. **M-ì¶”ì •ìì˜ í˜**: ì˜í–¥ í•¨ìˆ˜ë¥¼ ì œí•œí•˜ê±°ë‚˜ redescendingí•˜ê²Œ ë§Œë“¤ì–´ ì´ìƒì¹˜ì˜ ì˜í–¥ì„ í†µì œí•©ë‹ˆë‹¤.

3. **IRLSì˜ ìš°ì•„í•¨**: ë³µì¡í•œ robust ìµœì í™”ë¥¼ ë°˜ë³µì ì¸ ê°€ì¤‘ ìµœì†Œ ì œê³± ë¬¸ì œë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

4. **ì»¤ë„ ì„ íƒì˜ ì¤‘ìš”ì„±**:
   - **Huber**: ì•ˆì „í•œ ì²« ì„ íƒ, ë³¼ë¡ì„± ë³´ì¥
   - **Cauchy**: ê°•ë ¥í•œ ì´ìƒì¹˜ ì²˜ë¦¬, ì ë‹¹í•œ ë¹„ë³¼ë¡ì„±
   - **Tukey**: ê·¹ë‹¨ì  ì´ìƒì¹˜ ì™„ì „ ê±°ë¶€

5. **ìˆ˜ë ´ì„±ê³¼ ì•ˆì •ì„±**: ë³¼ë¡ ì»¤ë„ì€ ìˆ˜ë ´ ë³´ì¥, ë¹„ë³¼ë¡ ì»¤ë„ì€ GNCë‚˜ ì¢‹ì€ ì´ˆê¸°ê°’ í•„ìš”

### 10.2 ì‹¤ë¬´ ì²´í¬ë¦¬ìŠ¤íŠ¸

âœ… ì„¼ì„œë³„ ë…¸ì´ì¦ˆ íŠ¹ì„± íŒŒì•…  
âœ… MAD ê¸°ë°˜ robust scale ì¶”ì •  
âœ… ë‹¤ë‹¨ê³„ ìµœì í™” ì „ëµ ì ìš©  
âœ… ìˆ˜ë ´ì„± ëª¨ë‹ˆí„°ë§  
âœ… ì´ìƒì¹˜ ë¹„ìœ¨ ì¶”ì   

### 10.3 ë‹¤ìŒ ì¥ ì˜ˆê³ 

Chapter 7ì—ì„œëŠ” íšŒì „ í–‰ë ¬ì˜ íŠ¹ìˆ˜í•œ êµ¬ì¡°ë¥¼ í™œìš©í•œ **Chordal ì´ˆê¸°í™”**ë¥¼ ë°°ì›ë‹ˆë‹¤. ë¹„ì„ í˜• ìµœì í™”ì˜ ì¢‹ì€ ì´ˆê¸°ê°’ì„ êµ¬í•˜ëŠ” ê²ƒì´ ì™œ ì¤‘ìš”í•œì§€, ê·¸ë¦¬ê³  íšŒì „ì˜ ê¸°í•˜í•™ì  ì„±ì§ˆì„ ì–´ë–»ê²Œ í™œìš©í•˜ëŠ”ì§€ ì‚´í´ë³¼ ê²ƒì…ë‹ˆë‹¤.

**í•µì‹¬ ì§ˆë¬¸ ë˜ëŒì•„ë³´ê¸°:**
- âœ“ ì‹¤ì œ SLAMì˜ ì´ìƒì¹˜ëŠ” ì„¼ì„œì™€ í™˜ê²½ì˜ ë³¸ì§ˆì  í•œê³„
- âœ“ M-estimatorëŠ” MLEì˜ robust ì¼ë°˜í™”
- âœ“ IRLSëŠ” ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ë¡œ ë¹„ë³¼ë¡ì„± ì²˜ë¦¬
- âœ“ ê° kernelì€ íŠ¹ì • heavy-tailed ë¶„í¬ì— ëŒ€ì‘
- âœ“ ë³¼ë¡ ì»¤ë„ + ì¢‹ì€ ì´ˆê¸°ê°’ = ìˆ˜ë ´ì„± ë³´ì¥

ì´ì œ ì‹¤ìŠµì—ì„œ ì´ìƒì¹˜ê°€ í¬í•¨ëœ ë°ì´í„°ë¡œ ê° ë°©ë²•ì˜ íš¨ê³¼ë¥¼ ì§ì ‘ í™•ì¸í•´ë³´ì„¸ìš”!