# PGO 101 - Chapter 8 이론 강의: 대규모 SLAM을 위한 고급 기법

**강의 목표:** 이 강의를 마치면, 여러분은 소규모 테스트 환경을 넘어 실제 로봇이 넓은 공간을 장시간 탐사할 때 마주하는 문제들을 해결하는 고급 SLAM 기법들을 이해하게 됩니다. **루프 클로저 (Loop Closure)** 의 전체 파이프라인을 배우고, 실시간 성능을 보장하기 위한 **증분 최적화 (Incremental Optimization)**, 그리고 메모리와 계산 효율을 위한 **그래프 희소화 (Graph Sparsification)** 의 필요성과 원리를 설명할 수 있게 됩니다. 이 강의는 `chapter08_advanced_topics_loop_closure.ipynb`에서 완전한 SLAM 시스템을 시뮬레이션하기 위한 핵심 이론을 제공합니다.

---

## 1. 루프 클로저: SLAM의 구원투수

로봇이 계속 주행하면 오도메트리 (Odometry) 의 작은 오차들이 눈덩이처럼 불어나 결국 전체 궤적과 지도가 심하게 왜곡됩니다. 이를 **누적 오차 (Accumulated Drift)** 문제라고 합니다.

**루프 클로저 (Loop Closure)** 는 이 문제를 해결하는 가장 강력한 방법입니다.

> 💡 **핵심 아이디어**: 로봇이 주행 중 과거에 �����문했던 장소를 다시 인식했을 때, "아, 여기는 내가 30분 전에 있던 바로 그곳이구나!"라는 새로운 제약 조건을 그래프에 추가합니다. 이 강력한 제약 조건은 그동안 누적된 모든 오차를 바로잡는 '기준점' 역할을 하여, 전체 그래프를 전역적으로 일관성 있게 재조정합니다.

### 루프 클로저 파이프라인

루프 클로저는 단순히 장소를 찾는 것 이상의 과정입니다.

1.  **후보 탐지 (Candidate Detection)**: 현재 위치와 비슷해 보이는 과거의 장소들을 찾아냅니다.
    *   **방법**: 현재의 센서 데이터 (이미지, LiDAR 스캔) 를 과거 데이터들과 비교하거나, 현재 위치와 가까운 과거 포즈들을 KD-Tree와 같은 자료구조로 빠르게 검색합니다.
2.  **기하학적 검증 (Geometric Verification)**: 탐지된 후보가 정말로 같은 장소인지 정밀하게 검증합니다.
    *   **방법**: 두 위치 간의 상대 변환을 계산하고, 이것이 센서 측정값과 일치하는지 확인합니다 (예: ICP, RANSAC, Chi-squared test).
    *   **중요성**: **잘못된 루프 클로저 (False Positive) 는 올바른 루프 클로저가 없는 것보다 훨씬 더 나쁩니다.** 잘못된 제약 하나가 전체 지도를 완전히 망가뜨릴 수 있기 때��입��다.
3.  **그래프에 추가**: 검증된 루프 클로저를 새로운 간선 (Edge) 으로 포즈 그래프에 추가합니다.
4.  **전역 최적화 (Global Optimization)**: 새로운 루프 클로저 제약을 포함하여 전체 그래프를 다시 최적화합니다.

### [실습 연결]
`chapter08` 노트북의 **1. Loop Closure Detection and Validation** 섹션에서는, KD-Tree를 이용한 후보 탐지와 Chi-squared test를 이용한 검증 과정을 포함하는 `LoopClosureDetector` 클래스를 구현합니다.

---

## 2. 증분 최적화 (Incremental Optimization): 실시간을 위한 필수 전략

루프 클로저가 발생하면 전체 그래프를 다시 최적화해야 합니다. 하지만 그래프에 수천, 수만 개의 포즈가 있다면, 이 과정은 수 초에서 수 분이 걸려 실시간 처리가 불가능합니다.

**증분 최적화**는 이 문제를 해결하기 위한 기법으로, 전체 그래프를 매번 다시 계산하는 대신 **변화가 생긴 부분만 효율적으로 업데이트**합니다.

*   **슬라이딩 윈도우 (Sliding Window)**: 모든 포즈를 최적화하는 대신, 로봇의 현재 위치 주변의 최근 N개의 포즈 (활성 윈도우) 에 대해서만 최적화를 집중합니다.
*   **주변화 (Marginalization)**: 윈도우에서 밀려나는 오래된 포즈들은 최적화 변수에서 제거됩니다. 하지만 이 포즈들이 가지고 있던 정보는, 윈도우에 남아있는 포즈들에 대한 **사전 확률 (Prior)** 정보로 압축되어 전달됩니다. 이를 통해 과거의 정보를 완전히 잃지 않으면서도 계산량을 획기적으로 줄일 수 있습니다.

> 💡 **핵심 비유**: 여러분이 매우 긴 소설을 수정한다고 생각해보세요.
> - **배치 최적화**: 오타 하나를 발견할 때마다 책 전체를 처음부터 다시 읽고 수정하는 것.
> - **증분 최적화**: 오타가 발생한 문단과 그 주변 문맥만 수정하고, 나머지 챕터는 그대로 두는 것.

GTSAM의 **iSAM2** 는 이러한 증분 최적화를 매우 효율적으로 수행하는 대표적인 알고리즘입니다.

### [실습 연결]
`chapter08` 노트북의 **2. Incremental Pose Graph Optimization** 섹션에서는, SymForce의 Optimizer를 사용하여 활성 윈도우 내의 변수들만 최적화하는 `IncrementalPoseGraphOptimizer` 를 구현합니다.

---

## 3. 그래프 희소화 (Graph Sparsification): 그래프 다이어트

장시간 SLAM에서는 그래프의 크기가 계속 커져 메모리와 계산량 문제를 야기합니다. **그래프 희소화**는 그래프의 핵심적인 구조는 유지하면서 정보량이 적거나 중복되는 간선을 제거하여 그래프를 가볍게 만드는 기술입니다.

*   **전략**:
    *   루프 클로저와 같이 중요한 간선은 항상 유지합니다.
    *   그래프의 전체적인 연결성을 유지하는 뼈대 (예: **최대 신장 트리, Maximum Spanning Tree**) 는 보존합니다.
    *   나머지 간선들은 정보 이득 (Information Gain) 을 기준으로 점수가 낮은 것부터 제거합니다.

### [실습 연결]
`chapter08` 노트북의 **3. Graph Sparsification** 섹션에서는, `networkx` 라이브러리를 사용하여 그래프의 최대 신장 트리를 찾고, 이를 기반으로 그래프를 희소화하는 `GraphSparsifier` 를 구현합니다.