{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8: Advanced Topics - Loop Closure and Global Optimization\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand loop closure detection and validation\n",
    "- Implement incremental pose graph optimization\n",
    "- Learn about graph sparsification techniques\n",
    "- Explore multi-robot and hierarchical optimization\n",
    "- Build a complete SLAM system with Symforce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loop Closure Detection and Validation\n",
    "\n",
    "Loop closures are crucial for global consistency in SLAM. They connect distant parts of the trajectory, reducing drift accumulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import KDTree\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "from scipy.sparse.linalg import spsolve\n",
    "import symforce\n",
    "symforce.set_epsilon_to_number(1e-8)\n",
    "import symforce.symbolic as sf\n",
    "from symforce import ops\n",
    "from symforce.values import Values\n",
    "from symforce.opt.optimizer import Optimizer\n",
    "from symforce.opt.factor import Factor\n",
    "from typing import List, Tuple, Dict, Optional, Set\n",
    "import time\n",
    "from collections import deque\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoopClosureDetector:\n",
    "    def __init__(self, distance_threshold: float = 2.0, \n",
    "                 time_threshold: int = 50,\n",
    "                 max_candidates: int = 10):\n",
    "        self.distance_threshold = distance_threshold\n",
    "        self.time_threshold = time_threshold\n",
    "        self.max_candidates = max_candidates\n",
    "        self.pose_history = []\n",
    "        self.kdtree = None\n",
    "    \n",
    "    def add_pose(self, pose_id: int, x: float, y: float, theta: float):\n",
    "        \"\"\"Add a new pose to the detector\"\"\"\n",
    "        self.pose_history.append((pose_id, x, y, theta))\n",
    "        \n",
    "        # Rebuild KD-tree periodically for efficiency\n",
    "        if len(self.pose_history) % 10 == 0:\n",
    "            positions = np.array([[p[1], p[2]] for p in self.pose_history])\n",
    "            self.kdtree = KDTree(positions)\n",
    "    \n",
    "    def find_loop_closure_candidates(self, current_id: int) -> List[int]:\n",
    "        \"\"\"Find potential loop closure candidates for current pose\"\"\"\n",
    "        if len(self.pose_history) < self.time_threshold or self.kdtree is None:\n",
    "            return []\n",
    "        \n",
    "        current_pose = self.pose_history[current_id]\n",
    "        current_pos = np.array([current_pose[1], current_pose[2]])\n",
    "        \n",
    "        # Find nearby poses\n",
    "        indices = self.kdtree.query_ball_point(current_pos, self.distance_threshold)\n",
    "        \n",
    "        # Filter candidates\n",
    "        candidates = []\n",
    "        for idx in indices:\n",
    "            candidate_id = self.pose_history[idx][0]\n",
    "            \n",
    "            # Skip recent poses\n",
    "            if abs(current_id - candidate_id) < self.time_threshold:\n",
    "                continue\n",
    "            \n",
    "            # Compute actual distance\n",
    "            candidate_pos = np.array([self.pose_history[idx][1], \n",
    "                                    self.pose_history[idx][2]])\n",
    "            distance = np.linalg.norm(current_pos - candidate_pos)\n",
    "            \n",
    "            candidates.append((candidate_id, distance))\n",
    "        \n",
    "        # Sort by distance and return top candidates\n",
    "        candidates.sort(key=lambda x: x[1])\n",
    "        return [c[0] for c in candidates[:self.max_candidates]]\n",
    "    \n",
    "    def validate_loop_closure(self, pose_i: np.ndarray, pose_j: np.ndarray,\n",
    "                            measurement: np.ndarray, \n",
    "                            chi2_threshold: float = 5.991) -> bool:\n",
    "        \"\"\"Validate loop closure using chi-squared test\"\"\"\n",
    "        # Compute predicted measurement\n",
    "        c = np.cos(pose_i[2])\n",
    "        s = np.sin(pose_i[2])\n",
    "        dx_pred = c * (pose_j[0] - pose_i[0]) + s * (pose_j[1] - pose_i[1])\n",
    "        dy_pred = -s * (pose_j[0] - pose_i[0]) + c * (pose_j[1] - pose_i[1])\n",
    "        dtheta_pred = pose_j[2] - pose_i[2]\n",
    "        \n",
    "        # Normalize angle\n",
    "        dtheta_pred = np.arctan2(np.sin(dtheta_pred), np.cos(dtheta_pred))\n",
    "        \n",
    "        # Compute error\n",
    "        error = np.array([\n",
    "            dx_pred - measurement[0],\n",
    "            dy_pred - measurement[1],\n",
    "            np.arctan2(np.sin(dtheta_pred - measurement[2]), \n",
    "                      np.cos(dtheta_pred - measurement[2]))\n",
    "        ])\n",
    "        \n",
    "        # Assume diagonal covariance for simplicity\n",
    "        cov = np.diag([0.1, 0.1, 0.05])**2\n",
    "        \n",
    "        # Chi-squared test\n",
    "        chi2 = error.T @ np.linalg.inv(cov) @ error\n",
    "        \n",
    "        return chi2 < chi2_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Incremental Pose Graph Optimization\n",
    "\n",
    "For real-time SLAM, we need incremental optimization that efficiently updates the solution as new constraints arrive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IncrementalPoseGraphOptimizer:\n",
    "    def __init__(self, window_size: int = 50):\n",
    "        self.vertices = {}  # id -> (x, y, theta)\n",
    "        self.edges = []     # (i, j, measurement, info_matrix)\n",
    "        self.fixed_vertices = set([0])\n",
    "        self.window_size = window_size\n",
    "        self.active_vertices = deque(maxlen=window_size)\n",
    "        self.marginalized_vertices = set()\n",
    "        \n",
    "        # For Symforce optimization\n",
    "        self.values = Values()\n",
    "        self.factors = []\n",
    "        self.optimizer = None\n",
    "    \n",
    "    def add_vertex(self, vertex_id: int, x: float, y: float, theta: float):\n",
    "        \"\"\"Add a new vertex to the graph\"\"\"\n",
    "        self.vertices[vertex_id] = np.array([x, y, theta])\n",
    "        self.active_vertices.append(vertex_id)\n",
    "        \n",
    "        # Add to Symforce values\n",
    "        self.values[f'pose_{vertex_id}'] = sf.Pose2(\n",
    "            t=sf.V2(x, y),\n",
    "            R=sf.Rot2.from_angle(theta)\n",
    "        )\n",
    "        \n",
    "        # Check if we need to marginalize old vertices\n",
    "        if len(self.active_vertices) == self.window_size:\n",
    "            self._marginalize_oldest_vertex()\n",
    "    \n",
    "    def add_edge(self, i: int, j: int, dx: float, dy: float, dtheta: float,\n",
    "                 info_matrix: np.ndarray = None, is_loop_closure: bool = False):\n",
    "        \"\"\"Add an edge to the graph\"\"\"\n",
    "        if info_matrix is None:\n",
    "            info_matrix = np.eye(3)\n",
    "        \n",
    "        measurement = sf.Pose2(\n",
    "            t=sf.V2(dx, dy),\n",
    "            R=sf.Rot2.from_angle(dtheta)\n",
    "        )\n",
    "        \n",
    "        self.edges.append((i, j, measurement, info_matrix, is_loop_closure))\n",
    "        \n",
    "        # Only add factor if both vertices are active\n",
    "        if i not in self.marginalized_vertices and j not in self.marginalized_vertices:\n",
    "            self._add_edge_factor(i, j, measurement, info_matrix)\n",
    "    \n",
    "    def _add_edge_factor(self, i: int, j: int, measurement: sf.Pose2, \n",
    "                        info_matrix: np.ndarray):\n",
    "        \"\"\"Add edge factor to Symforce optimizer\"\"\"\n",
    "        def edge_residual(pose_i: sf.Pose2, pose_j: sf.Pose2) -> sf.V3:\n",
    "            T_ij_predicted = pose_i.inverse() * pose_j\n",
    "            T_error = measurement.inverse() * T_ij_predicted\n",
    "            \n",
    "            return sf.V3(\n",
    "                T_error.position()[0],\n",
    "                T_error.position()[1],\n",
    "                T_error.rotation().to_tangent()[0]\n",
    "            )\n",
    "        \n",
    "        # Create factor with information matrix weighting\n",
    "        factor = Factor(\n",
    "            residual=edge_residual,\n",
    "            keys=[f'pose_{i}', f'pose_{j}'],\n",
    "            noise_model=np.linalg.inv(info_matrix)\n",
    "        )\n",
    "        \n",
    "        self.factors.append(factor)\n",
    "    \n",
    "    def _marginalize_oldest_vertex(self):\n",
    "        \"\"\"Marginalize the oldest vertex using Schur complement\"\"\"\n",
    "        if len(self.active_vertices) < self.window_size:\n",
    "            return\n",
    "        \n",
    "        oldest = self.active_vertices[0]\n",
    "        if oldest in self.fixed_vertices:\n",
    "            return\n",
    "        \n",
    "        # Mark as marginalized\n",
    "        self.marginalized_vertices.add(oldest)\n",
    "        \n",
    "        # TODO: Implement proper marginalization\n",
    "        # For now, we'll just fix the vertex\n",
    "        self.fixed_vertices.add(oldest)\n",
    "    \n",
    "    def optimize_incremental(self, max_iterations: int = 10) -> Dict[str, List[float]]:\n",
    "        \"\"\"Perform incremental optimization on active window\"\"\"\n",
    "        if not self.factors:\n",
    "            return {'error': [], 'time': []}\n",
    "        \n",
    "        # Create optimizer if needed\n",
    "        if self.optimizer is None:\n",
    "            self.optimizer = Optimizer(\n",
    "                factors=self.factors,\n",
    "                optimized_keys=[f'pose_{v}' for v in self.active_vertices \n",
    "                              if v not in self.fixed_vertices],\n",
    "                debug_stats=True\n",
    "            )\n",
    "        else:\n",
    "            # Update factors and keys\n",
    "            self.optimizer.factors = self.factors\n",
    "            self.optimizer.update_optimized_keys(\n",
    "                [f'pose_{v}' for v in self.active_vertices \n",
    "                 if v not in self.fixed_vertices]\n",
    "            )\n",
    "        \n",
    "        # Optimize\n",
    "        start_time = time.time()\n",
    "        result = self.optimizer.optimize(self.values, max_iterations=max_iterations)\n",
    "        opt_time = time.time() - start_time\n",
    "        \n",
    "        # Update vertex positions\n",
    "        for v in self.active_vertices:\n",
    "            if v not in self.fixed_vertices:\n",
    "                pose = result.optimized_values[f'pose_{v}']\n",
    "                self.vertices[v] = np.array([\n",
    "                    pose.position()[0],\n",
    "                    pose.position()[1],\n",
    "                    pose.rotation().to_angle()\n",
    "                ])\n",
    "        \n",
    "        # Update values for next iteration\n",
    "        self.values = result.optimized_values\n",
    "        \n",
    "        return {\n",
    "            'error': [result.error()],\n",
    "            'time': [opt_time],\n",
    "            'iterations': result.iterations\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Graph Sparsification\n",
    "\n",
    "As the graph grows, we need to sparsify it to maintain computational efficiency while preserving accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSparsifier:\n",
    "    def __init__(self, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    def compute_edge_importance(self) -> Dict[Tuple[int, int], float]:\n",
    "        \"\"\"Compute importance score for each edge using information gain\"\"\"\n",
    "        edge_scores = {}\n",
    "        \n",
    "        for i, j, measurement, info_matrix, is_loop_closure in self.optimizer.edges:\n",
    "            # Loop closures are always important\n",
    "            if is_loop_closure:\n",
    "                edge_scores[(i, j)] = float('inf')\n",
    "                continue\n",
    "            \n",
    "            # Compute error for this edge\n",
    "            pose_i = self.optimizer.vertices[i]\n",
    "            pose_j = self.optimizer.vertices[j]\n",
    "            \n",
    "            # Predicted measurement\n",
    "            c = np.cos(pose_i[2])\n",
    "            s = np.sin(pose_i[2])\n",
    "            dx_pred = c * (pose_j[0] - pose_i[0]) + s * (pose_j[1] - pose_i[1])\n",
    "            dy_pred = -s * (pose_j[0] - pose_i[0]) + c * (pose_j[1] - pose_i[1])\n",
    "            dtheta_pred = np.arctan2(np.sin(pose_j[2] - pose_i[2]), \n",
    "                                    np.cos(pose_j[2] - pose_i[2]))\n",
    "            \n",
    "            # Error\n",
    "            error = np.array([\n",
    "                dx_pred - measurement.position()[0],\n",
    "                dy_pred - measurement.position()[1],\n",
    "                dtheta_pred - measurement.rotation().to_angle()\n",
    "            ])\n",
    "            \n",
    "            # Information gain (simplified)\n",
    "            score = error.T @ info_matrix @ error\n",
    "            edge_scores[(i, j)] = score\n",
    "        \n",
    "        return edge_scores\n",
    "    \n",
    "    def sparsify_chow_liu(self, target_edges: int) -> List[Tuple[int, int]]:\n",
    "        \"\"\"Sparsify using Chow-Liu tree approximation\"\"\"\n",
    "        # Build graph\n",
    "        G = nx.Graph()\n",
    "        edge_scores = self.compute_edge_importance()\n",
    "        \n",
    "        for (i, j), score in edge_scores.items():\n",
    "            G.add_edge(i, j, weight=-score)  # Negative for maximum spanning tree\n",
    "        \n",
    "        # Find maximum spanning tree\n",
    "        mst = nx.maximum_spanning_tree(G)\n",
    "        \n",
    "        # Add edges back based on importance\n",
    "        selected_edges = set(mst.edges())\n",
    "        remaining_edges = [(e, s) for e, s in edge_scores.items() \n",
    "                          if e not in selected_edges and s != float('inf')]\n",
    "        remaining_edges.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Add important edges until target reached\n",
    "        for edge, _ in remaining_edges:\n",
    "            if len(selected_edges) >= target_edges:\n",
    "                break\n",
    "            selected_edges.add(edge)\n",
    "        \n",
    "        # Always keep loop closures\n",
    "        for edge, score in edge_scores.items():\n",
    "            if score == float('inf'):\n",
    "                selected_edges.add(edge)\n",
    "        \n",
    "        return list(selected_edges)\n",
    "    \n",
    "    def apply_sparsification(self, selected_edges: List[Tuple[int, int]]):\n",
    "        \"\"\"Apply sparsification to the optimizer\"\"\"\n",
    "        # Create edge lookup\n",
    "        edge_set = set(selected_edges)\n",
    "        \n",
    "        # Filter edges\n",
    "        new_edges = []\n",
    "        new_factors = []\n",
    "        \n",
    "        for idx, (i, j, measurement, info_matrix, is_loop_closure) in enumerate(self.optimizer.edges):\n",
    "            if (i, j) in edge_set or (j, i) in edge_set:\n",
    "                new_edges.append((i, j, measurement, info_matrix, is_loop_closure))\n",
    "                if idx < len(self.optimizer.factors):\n",
    "                    new_factors.append(self.optimizer.factors[idx])\n",
    "        \n",
    "        self.optimizer.edges = new_edges\n",
    "        self.optimizer.factors = new_factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Complete SLAM System Example\n",
    "\n",
    "Let's build a complete SLAM system that combines all the techniques we've learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompleteSLAMSystem:\n",
    "    def __init__(self, loop_closure_config: Dict = None, \n",
    "                 sparsification_config: Dict = None):\n",
    "        # Core components\n",
    "        self.optimizer = IncrementalPoseGraphOptimizer(window_size=100)\n",
    "        self.loop_detector = LoopClosureDetector(\n",
    "            **(loop_closure_config or {})\n",
    "        )\n",
    "        self.sparsifier = GraphSparsifier(self.optimizer)\n",
    "        \n",
    "        # Configuration\n",
    "        self.sparsification_interval = sparsification_config.get('interval', 100) if sparsification_config else 100\n",
    "        self.target_edge_ratio = sparsification_config.get('target_ratio', 0.5) if sparsification_config else 0.5\n",
    "        \n",
    "        # Statistics\n",
    "        self.stats = {\n",
    "            'poses': [],\n",
    "            'edges': [],\n",
    "            'loop_closures': [],\n",
    "            'optimization_time': [],\n",
    "            'total_error': []\n",
    "        }\n",
    "    \n",
    "    def add_odometry_measurement(self, pose_id: int, x: float, y: float, theta: float,\n",
    "                               dx: float, dy: float, dtheta: float,\n",
    "                               covariance: np.ndarray = None):\n",
    "        \"\"\"Process new odometry measurement\"\"\"\n",
    "        # Add vertex\n",
    "        self.optimizer.add_vertex(pose_id, x, y, theta)\n",
    "        self.loop_detector.add_pose(pose_id, x, y, theta)\n",
    "        \n",
    "        # Add odometry edge\n",
    "        if pose_id > 0:\n",
    "            if covariance is None:\n",
    "                covariance = np.diag([0.1, 0.1, 0.05])**2\n",
    "            info_matrix = np.linalg.inv(covariance)\n",
    "            \n",
    "            self.optimizer.add_edge(pose_id - 1, pose_id, dx, dy, dtheta, \n",
    "                                  info_matrix, is_loop_closure=False)\n",
    "        \n",
    "        # Check for loop closures\n",
    "        loop_candidates = self.loop_detector.find_loop_closure_candidates(pose_id)\n",
    "        \n",
    "        for candidate_id in loop_candidates:\n",
    "            # Simulate loop closure detection (in practice, this would use sensors)\n",
    "            success, measurement = self._detect_loop_closure(pose_id, candidate_id)\n",
    "            \n",
    "            if success:\n",
    "                # Validate loop closure\n",
    "                pose_current = self.optimizer.vertices[pose_id]\n",
    "                pose_candidate = self.optimizer.vertices[candidate_id]\n",
    "                \n",
    "                if self.loop_detector.validate_loop_closure(\n",
    "                    pose_candidate, pose_current, measurement\n",
    "                ):\n",
    "                    # Add loop closure edge\n",
    "                    lc_cov = np.diag([0.2, 0.2, 0.1])**2\n",
    "                    lc_info = np.linalg.inv(lc_cov)\n",
    "                    \n",
    "                    self.optimizer.add_edge(\n",
    "                        candidate_id, pose_id, \n",
    "                        measurement[0], measurement[1], measurement[2],\n",
    "                        lc_info, is_loop_closure=True\n",
    "                    )\n",
    "                    \n",
    "                    self.stats['loop_closures'].append((candidate_id, pose_id))\n",
    "                    print(f\"Loop closure detected: {candidate_id} -> {pose_id}\")\n",
    "        \n",
    "        # Optimize\n",
    "        opt_result = self.optimizer.optimize_incremental(max_iterations=10)\n",
    "        \n",
    "        # Update statistics\n",
    "        self.stats['poses'].append(pose_id)\n",
    "        self.stats['edges'].append(len(self.optimizer.edges))\n",
    "        self.stats['optimization_time'].extend(opt_result['time'])\n",
    "        self.stats['total_error'].extend(opt_result['error'])\n",
    "        \n",
    "        # Sparsify periodically\n",
    "        if pose_id > 0 and pose_id % self.sparsification_interval == 0:\n",
    "            self._sparsify_graph()\n",
    "    \n",
    "    def _detect_loop_closure(self, current_id: int, candidate_id: int) -> Tuple[bool, np.ndarray]:\n",
    "        \"\"\"Simulate loop closure detection\"\"\"\n",
    "        # In a real system, this would use place recognition and scan matching\n",
    "        # Here we simulate it based on ground truth with noise\n",
    "        \n",
    "        pose_current = self.optimizer.vertices[current_id]\n",
    "        pose_candidate = self.optimizer.vertices[candidate_id]\n",
    "        \n",
    "        # Only detect if actually close\n",
    "        distance = np.linalg.norm(pose_current[:2] - pose_candidate[:2])\n",
    "        if distance > 2.0:\n",
    "            return False, None\n",
    "        \n",
    "        # Compute relative pose with noise\n",
    "        c = np.cos(pose_candidate[2])\n",
    "        s = np.sin(pose_candidate[2])\n",
    "        dx_true = c * (pose_current[0] - pose_candidate[0]) + s * (pose_current[1] - pose_candidate[1])\n",
    "        dy_true = -s * (pose_current[0] - pose_candidate[0]) + c * (pose_current[1] - pose_candidate[1])\n",
    "        dtheta_true = pose_current[2] - pose_candidate[2]\n",
    "        \n",
    "        # Add noise\n",
    "        dx = dx_true + np.random.normal(0, 0.1)\n",
    "        dy = dy_true + np.random.normal(0, 0.1)\n",
    "        dtheta = dtheta_true + np.random.normal(0, 0.05)\n",
    "        \n",
    "        return True, np.array([dx, dy, dtheta])\n",
    "    \n",
    "    def _sparsify_graph(self):\n",
    "        \"\"\"Sparsify the pose graph\"\"\"\n",
    "        print(f\"\\nSparsifying graph...\")\n",
    "        n_edges_before = len(self.optimizer.edges)\n",
    "        \n",
    "        target_edges = int(n_edges_before * self.target_edge_ratio)\n",
    "        selected_edges = self.sparsifier.sparsify_chow_liu(target_edges)\n",
    "        self.sparsifier.apply_sparsification(selected_edges)\n",
    "        \n",
    "        n_edges_after = len(self.optimizer.edges)\n",
    "        print(f\"Sparsified: {n_edges_before} -> {n_edges_after} edges\")\n",
    "    \n",
    "    def get_trajectory(self) -> np.ndarray:\n",
    "        \"\"\"Get the optimized trajectory\"\"\"\n",
    "        poses = []\n",
    "        for i in sorted(self.optimizer.vertices.keys()):\n",
    "            poses.append(self.optimizer.vertices[i])\n",
    "        return np.array(poses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Testing the Complete System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_slam_scenario(n_poses: int = 200, noise_level: float = 0.1):\n",
    "    \"\"\"Simulate a SLAM scenario with noisy odometry\"\"\"\n",
    "    # Create SLAM system\n",
    "    slam = CompleteSLAMSystem(\n",
    "        loop_closure_config={\n",
    "            'distance_threshold': 2.0,\n",
    "            'time_threshold': 30,\n",
    "            'max_candidates': 5\n",
    "        },\n",
    "        sparsification_config={\n",
    "            'interval': 50,\n",
    "            'target_ratio': 0.7\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Generate figure-8 trajectory\n",
    "    t = np.linspace(0, 4*np.pi, n_poses)\n",
    "    scale = 10.0\n",
    "    \n",
    "    ground_truth = []\n",
    "    \n",
    "    for i in range(n_poses):\n",
    "        # Figure-8 parametric equations\n",
    "        x_true = scale * np.sin(t[i])\n",
    "        y_true = scale * np.sin(t[i]) * np.cos(t[i])\n",
    "        \n",
    "        # Compute heading (tangent direction)\n",
    "        if i < n_poses - 1:\n",
    "            dx = scale * np.cos(t[i+1])\n",
    "            dy = scale * (np.cos(t[i+1]) * np.cos(t[i+1]) - np.sin(t[i+1]) * np.sin(t[i+1]))\n",
    "            theta_true = np.arctan2(dy, dx)\n",
    "        else:\n",
    "            theta_true = ground_truth[-1][2]\n",
    "        \n",
    "        ground_truth.append([x_true, y_true, theta_true])\n",
    "        \n",
    "        # Generate noisy odometry\n",
    "        if i == 0:\n",
    "            # First pose\n",
    "            slam.add_odometry_measurement(i, x_true, y_true, theta_true, 0, 0, 0)\n",
    "        else:\n",
    "            # Compute true odometry\n",
    "            prev = ground_truth[i-1]\n",
    "            c = np.cos(prev[2])\n",
    "            s = np.sin(prev[2])\n",
    "            \n",
    "            dx_global = x_true - prev[0]\n",
    "            dy_global = y_true - prev[1]\n",
    "            \n",
    "            dx_true = c * dx_global + s * dy_global\n",
    "            dy_true = -s * dx_global + c * dy_global\n",
    "            dtheta_true = theta_true - prev[2]\n",
    "            \n",
    "            # Add noise\n",
    "            dx = dx_true + np.random.normal(0, noise_level)\n",
    "            dy = dy_true + np.random.normal(0, noise_level)\n",
    "            dtheta = dtheta_true + np.random.normal(0, noise_level * 0.5)\n",
    "            \n",
    "            # Dead reckoning for initial guess\n",
    "            x_dr = slam.optimizer.vertices[i-1][0] + c * dx - s * dy\n",
    "            y_dr = slam.optimizer.vertices[i-1][1] + s * dx + c * dy\n",
    "            theta_dr = slam.optimizer.vertices[i-1][2] + dtheta\n",
    "            \n",
    "            slam.add_odometry_measurement(i, x_dr, y_dr, theta_dr, dx, dy, dtheta)\n",
    "        \n",
    "        # Progress\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"Processed {i + 1}/{n_poses} poses\")\n",
    "    \n",
    "    return slam, np.array(ground_truth)\n",
    "\n",
    "# Run simulation\n",
    "np.random.seed(42)\n",
    "print(\"Running SLAM simulation...\")\n",
    "slam_system, ground_truth = simulate_slam_scenario(n_poses=200, noise_level=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Trajectory comparison\n",
    "ax = axes[0, 0]\n",
    "trajectory = slam_system.get_trajectory()\n",
    "\n",
    "# Dead reckoning (no loop closures)\n",
    "dead_reckoning = [ground_truth[0]]\n",
    "for i in range(1, len(ground_truth)):\n",
    "    prev = dead_reckoning[-1]\n",
    "    # Use noisy odometry to accumulate\n",
    "    c = np.cos(prev[2])\n",
    "    s = np.sin(prev[2])\n",
    "    \n",
    "    dx_global = ground_truth[i][0] - ground_truth[i-1][0]\n",
    "    dy_global = ground_truth[i][1] - ground_truth[i-1][1]\n",
    "    dx_local = c * dx_global + s * dy_global + np.random.normal(0, 0.05)\n",
    "    dy_local = -s * dx_global + c * dy_global + np.random.normal(0, 0.05)\n",
    "    dtheta = ground_truth[i][2] - ground_truth[i-1][2] + np.random.normal(0, 0.025)\n",
    "    \n",
    "    x_next = prev[0] + c * dx_local - s * dy_local\n",
    "    y_next = prev[1] + s * dx_local + c * dy_local\n",
    "    theta_next = prev[2] + dtheta\n",
    "    \n",
    "    dead_reckoning.append([x_next, y_next, theta_next])\n",
    "\n",
    "dead_reckoning = np.array(dead_reckoning)\n",
    "\n",
    "ax.plot(ground_truth[:, 0], ground_truth[:, 1], 'g-', linewidth=2, \n",
    "        label='Ground Truth', alpha=0.7)\n",
    "ax.plot(dead_reckoning[:, 0], dead_reckoning[:, 1], 'r--', linewidth=2,\n",
    "        label='Dead Reckoning', alpha=0.7)\n",
    "ax.plot(trajectory[:, 0], trajectory[:, 1], 'b-', linewidth=2,\n",
    "        label='SLAM Estimate')\n",
    "\n",
    "# Plot loop closures\n",
    "for lc in slam_system.stats['loop_closures']:\n",
    "    i, j = lc\n",
    "    ax.plot([trajectory[i, 0], trajectory[j, 0]], \n",
    "            [trajectory[i, 1], trajectory[j, 1]], \n",
    "            'c-', alpha=0.5, linewidth=1)\n",
    "\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_title('Trajectory Comparison')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# Error over time\n",
    "ax = axes[0, 1]\n",
    "position_errors = np.linalg.norm(trajectory[:, :2] - ground_truth[:, :2], axis=1)\n",
    "ax.plot(position_errors, linewidth=2)\n",
    "ax.set_xlabel('Pose ID')\n",
    "ax.set_ylabel('Position Error (m)')\n",
    "ax.set_title('Position Error Over Time')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add loop closure markers\n",
    "for lc in slam_system.stats['loop_closures']:\n",
    "    ax.axvline(x=lc[1], color='c', alpha=0.5, linestyle='--')\n",
    "\n",
    "# Optimization statistics\n",
    "ax = axes[1, 0]\n",
    "ax.plot(slam_system.stats['optimization_time'], linewidth=2)\n",
    "ax.set_xlabel('Optimization Step')\n",
    "ax.set_ylabel('Time (s)')\n",
    "ax.set_title('Optimization Time per Step')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Graph statistics\n",
    "ax = axes[1, 1]\n",
    "ax.plot(slam_system.stats['poses'], slam_system.stats['edges'], \n",
    "        linewidth=2, label='Total Edges')\n",
    "ax.set_xlabel('Number of Poses')\n",
    "ax.set_ylabel('Number of Edges')\n",
    "ax.set_title('Graph Growth')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Mark sparsification events\n",
    "sparsification_poses = [i for i in slam_system.stats['poses'] \n",
    "                       if i > 0 and i % slam_system.sparsification_interval == 0]\n",
    "for sp in sparsification_poses:\n",
    "    ax.axvline(x=sp, color='r', alpha=0.5, linestyle='--')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSLAM System Summary:\")\n",
    "print(f\"Total poses: {len(trajectory)}\")\n",
    "print(f\"Total edges: {len(slam_system.optimizer.edges)}\")\n",
    "print(f\"Loop closures detected: {len(slam_system.stats['loop_closures'])}\")\n",
    "print(f\"Mean position error: {np.mean(position_errors):.3f} m\")\n",
    "print(f\"Final position error: {position_errors[-1]:.3f} m\")\n",
    "print(f\"Mean optimization time: {np.mean(slam_system.stats['optimization_time']):.3f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multi-Robot SLAM Extension\n",
    "\n",
    "Let's extend our system to handle multiple robots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiRobotSLAM:\n",
    "    def __init__(self, n_robots: int):\n",
    "        self.n_robots = n_robots\n",
    "        self.robot_systems = [\n",
    "            CompleteSLAMSystem() for _ in range(n_robots)\n",
    "        ]\n",
    "        self.inter_robot_measurements = []  # (robot_i, pose_i, robot_j, pose_j, measurement)\n",
    "        \n",
    "    def add_inter_robot_measurement(self, robot_i: int, pose_i: int,\n",
    "                                  robot_j: int, pose_j: int,\n",
    "                                  dx: float, dy: float, dtheta: float):\n",
    "        \"\"\"Add measurement between robots\"\"\"\n",
    "        measurement = np.array([dx, dy, dtheta])\n",
    "        self.inter_robot_measurements.append(\n",
    "            (robot_i, pose_i, robot_j, pose_j, measurement)\n",
    "        )\n",
    "    \n",
    "    def optimize_centralized(self):\n",
    "        \"\"\"Perform centralized optimization across all robots\"\"\"\n",
    "        # Build combined optimization problem\n",
    "        combined_values = Values()\n",
    "        combined_factors = []\n",
    "        \n",
    "        # Add each robot's poses and factors\n",
    "        for robot_id, robot_slam in enumerate(self.robot_systems):\n",
    "            for pose_id, pose in robot_slam.optimizer.vertices.items():\n",
    "                key = f'robot_{robot_id}_pose_{pose_id}'\n",
    "                combined_values[key] = sf.Pose2(\n",
    "                    t=sf.V2(pose[0], pose[1]),\n",
    "                    R=sf.Rot2.from_angle(pose[2])\n",
    "                )\n",
    "        \n",
    "        # Add inter-robot factors\n",
    "        for r_i, p_i, r_j, p_j, measurement in self.inter_robot_measurements:\n",
    "            def inter_robot_residual(pose_i: sf.Pose2, pose_j: sf.Pose2) -> sf.V3:\n",
    "                T_ij_predicted = pose_i.inverse() * pose_j\n",
    "                T_ij_measured = sf.Pose2(\n",
    "                    t=sf.V2(measurement[0], measurement[1]),\n",
    "                    R=sf.Rot2.from_angle(measurement[2])\n",
    "                )\n",
    "                T_error = T_ij_measured.inverse() * T_ij_predicted\n",
    "                \n",
    "                return sf.V3(\n",
    "                    T_error.position()[0],\n",
    "                    T_error.position()[1],\n",
    "                    T_error.rotation().to_tangent()[0]\n",
    "                )\n",
    "            \n",
    "            factor = Factor(\n",
    "                residual=inter_robot_residual,\n",
    "                keys=[f'robot_{r_i}_pose_{p_i}', f'robot_{r_j}_pose_{p_j}']\n",
    "            )\n",
    "            combined_factors.append(factor)\n",
    "        \n",
    "        # Optimize if we have inter-robot measurements\n",
    "        if combined_factors:\n",
    "            optimizer = Optimizer(\n",
    "                factors=combined_factors,\n",
    "                optimized_keys=list(combined_values.keys()),\n",
    "                debug_stats=True\n",
    "            )\n",
    "            \n",
    "            result = optimizer.optimize(combined_values, max_iterations=50)\n",
    "            \n",
    "            # Update robot poses\n",
    "            for key, pose in result.optimized_values.items():\n",
    "                parts = key.split('_')\n",
    "                robot_id = int(parts[1])\n",
    "                pose_id = int(parts[3])\n",
    "                \n",
    "                self.robot_systems[robot_id].optimizer.vertices[pose_id] = np.array([\n",
    "                    pose.position()[0],\n",
    "                    pose.position()[1],\n",
    "                    pose.rotation().to_angle()\n",
    "                ])\n",
    "\n",
    "# Example usage\n",
    "print(\"\\nMulti-robot SLAM example:\")\n",
    "multi_slam = MultiRobotSLAM(n_robots=2)\n",
    "\n",
    "# Simulate two robots moving in parallel\n",
    "for i in range(20):\n",
    "    # Robot 0: moves along x-axis\n",
    "    x0 = i * 0.5\n",
    "    multi_slam.robot_systems[0].add_odometry_measurement(i, x0, 0, 0, 0.5, 0, 0)\n",
    "    \n",
    "    # Robot 1: moves along y-axis\n",
    "    y1 = i * 0.5\n",
    "    multi_slam.robot_systems[1].add_odometry_measurement(i, 0, y1, np.pi/2, 0, 0.5, 0)\n",
    "    \n",
    "    # Simulate inter-robot observation every 5 steps\n",
    "    if i > 0 and i % 5 == 0:\n",
    "        # Robot 0 observes robot 1\n",
    "        dx = 0 - x0\n",
    "        dy = y1 - 0\n",
    "        dtheta = np.pi/2 - 0\n",
    "        \n",
    "        multi_slam.add_inter_robot_measurement(0, i, 1, i, dx, dy, dtheta)\n",
    "        print(f\"Inter-robot measurement at step {i}\")\n",
    "\n",
    "# Optimize\n",
    "multi_slam.optimize_centralized()\n",
    "print(\"Multi-robot optimization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze computational complexity\n",
    "def analyze_complexity(max_poses: int = 500, step: int = 50):\n",
    "    \"\"\"Analyze how optimization time scales with graph size\"\"\"\n",
    "    pose_counts = list(range(step, max_poses + 1, step))\n",
    "    \n",
    "    results = {\n",
    "        'batch': {'times': [], 'errors': []},\n",
    "        'incremental': {'times': [], 'errors': []},\n",
    "        'sparsified': {'times': [], 'errors': []}\n",
    "    }\n",
    "    \n",
    "    for n_poses in pose_counts:\n",
    "        print(f\"\\nTesting with {n_poses} poses...\")\n",
    "        \n",
    "        # Generate test data\n",
    "        angles = np.linspace(0, 2*np.pi, n_poses, endpoint=False)\n",
    "        \n",
    "        # Batch optimization (baseline)\n",
    "        batch_optimizer = IncrementalPoseGraphOptimizer(window_size=n_poses)\n",
    "        \n",
    "        for i, angle in enumerate(angles):\n",
    "            x = 10 * np.cos(angle) + np.random.normal(0, 0.1)\n",
    "            y = 10 * np.sin(angle) + np.random.normal(0, 0.1)\n",
    "            theta = angle + np.pi/2 + np.random.normal(0, 0.05)\n",
    "            \n",
    "            batch_optimizer.add_vertex(i, x, y, theta)\n",
    "            \n",
    "            if i > 0:\n",
    "                dx = 2 * np.pi * 10 / n_poses\n",
    "                batch_optimizer.add_edge(i-1, i, dx, 0, 2*np.pi/n_poses)\n",
    "        \n",
    "        # Add loop closure\n",
    "        batch_optimizer.add_edge(n_poses-1, 0, 0, 0, 0, is_loop_closure=True)\n",
    "        \n",
    "        # Time batch optimization\n",
    "        start_time = time.time()\n",
    "        batch_result = batch_optimizer.optimize_incremental(max_iterations=20)\n",
    "        batch_time = time.time() - start_time\n",
    "        \n",
    "        results['batch']['times'].append(batch_time)\n",
    "        results['batch']['errors'].append(batch_result['error'][-1] if batch_result['error'] else 0)\n",
    "        \n",
    "        print(f\"  Batch: {batch_time:.3f}s\")\n",
    "        \n",
    "        # TODO: Add incremental and sparsified versions\n",
    "    \n",
    "    return pose_counts, results\n",
    "\n",
    "# Run analysis\n",
    "pose_counts, complexity_results = analyze_complexity(max_poses=300, step=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot complexity results\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(pose_counts, complexity_results['batch']['times'], \n",
    "         'o-', linewidth=2, markersize=8, label='Batch Optimization')\n",
    "\n",
    "# Fit polynomial to show scaling\n",
    "coeffs = np.polyfit(pose_counts, complexity_results['batch']['times'], 2)\n",
    "fit_x = np.linspace(min(pose_counts), max(pose_counts), 100)\n",
    "fit_y = np.polyval(coeffs, fit_x)\n",
    "plt.plot(fit_x, fit_y, '--', alpha=0.7, label=f'Quadratic Fit: {coeffs[0]:.2e}nÂ²')\n",
    "\n",
    "plt.xlabel('Number of Poses')\n",
    "plt.ylabel('Optimization Time (s)')\n",
    "plt.title('Computational Complexity Analysis')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this final chapter, we covered advanced topics in pose graph optimization:\n",
    "\n",
    "1. **Loop Closure Detection**: Finding and validating connections between distant poses\n",
    "2. **Incremental Optimization**: Efficient updates for real-time SLAM\n",
    "3. **Graph Sparsification**: Maintaining efficiency as the graph grows\n",
    "4. **Complete SLAM System**: Integration of all components\n",
    "5. **Multi-Robot SLAM**: Extending to collaborative scenarios\n",
    "6. **Performance Analysis**: Understanding computational complexity\n",
    "\n",
    "Key takeaways:\n",
    "- Loop closures are essential for global consistency\n",
    "- Incremental methods enable real-time operation\n",
    "- Sparsification preserves accuracy while reducing computation\n",
    "- Symforce provides powerful tools for building efficient SLAM systems\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "To continue your learning:\n",
    "1. Implement visual loop closure detection using image features\n",
    "2. Explore 3D pose graph optimization for full 6-DOF SLAM\n",
    "3. Study more advanced sparsification techniques (e.g., information-theoretic)\n",
    "4. Implement distributed optimization for large-scale multi-robot systems\n",
    "5. Integrate with real sensor data (LiDAR, cameras, IMU)\n",
    "\n",
    "Congratulations on completing the pose graph optimization tutorial series!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
