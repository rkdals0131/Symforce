{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Robust Optimization with Cauchy Kernel\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand why robust optimization is necessary in SLAM\n",
    "- Implement various robust loss functions (Cauchy, Huber, Tukey)\n",
    "- Compare the effect of different kernels on optimization\n",
    "- Handle outliers in pose graph optimization\n",
    "- Visualize the impact of robust kernels on convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Robust Optimization?\n",
    "\n",
    "In real-world SLAM applications, measurements often contain outliers due to:\n",
    "- Incorrect loop closure detections\n",
    "- Sensor noise and failures\n",
    "- Dynamic objects in the environment\n",
    "- Data association errors\n",
    "\n",
    "Standard least-squares optimization is highly sensitive to outliers because it uses squared errors, giving large errors disproportionate influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import spsolve\n",
    "import symforce\n",
    "symforce.set_epsilon_to_number(1e-8)\n",
    "import symforce.symbolic as sf\n",
    "from symforce import ops\n",
    "from symforce.values import Values\n",
    "from symforce.opt.optimizer import Optimizer\n",
    "from symforce.opt.factor import Factor\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Robust Loss Functions\n",
    "\n",
    "Robust loss functions modify the squared error to reduce the influence of outliers:\n",
    "\n",
    "- **L2 (Standard)**: $\\rho(e) = \\frac{1}{2}e^2$\n",
    "- **Huber**: $\\rho(e) = \\begin{cases} \\frac{1}{2}e^2 & |e| \\leq \\delta \\\\ \\delta|e| - \\frac{1}{2}\\delta^2 & |e| > \\delta \\end{cases}$\n",
    "- **Cauchy**: $\\rho(e) = \\frac{c^2}{2} \\log\\left(1 + \\frac{e^2}{c^2}\\right)$\n",
    "- **Tukey**: $\\rho(e) = \\begin{cases} \\frac{c^2}{6}\\left(1 - \\left(1 - \\frac{e^2}{c^2}\\right)^3\\right) & |e| \\leq c \\\\ \\frac{c^2}{6} & |e| > c \\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustKernel:\n",
    "    \"\"\"Base class for robust kernels\"\"\"\n",
    "    def __init__(self, delta: float = 1.0):\n",
    "        self.delta = delta\n",
    "    \n",
    "    def weight(self, error: float) -> float:\n",
    "        \"\"\"Compute weight for weighted least squares\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def loss(self, error: float) -> float:\n",
    "        \"\"\"Compute robust loss\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class L2Kernel(RobustKernel):\n",
    "    \"\"\"Standard L2 loss (no robustness)\"\"\"\n",
    "    def weight(self, error: float) -> float:\n",
    "        return 1.0\n",
    "    \n",
    "    def loss(self, error: float) -> float:\n",
    "        return 0.5 * error**2\n",
    "\n",
    "class HuberKernel(RobustKernel):\n",
    "    \"\"\"Huber robust kernel\"\"\"\n",
    "    def weight(self, error: float) -> float:\n",
    "        abs_error = abs(error)\n",
    "        if abs_error <= self.delta:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return self.delta / abs_error\n",
    "    \n",
    "    def loss(self, error: float) -> float:\n",
    "        abs_error = abs(error)\n",
    "        if abs_error <= self.delta:\n",
    "            return 0.5 * error**2\n",
    "        else:\n",
    "            return self.delta * abs_error - 0.5 * self.delta**2\n",
    "\n",
    "class CauchyKernel(RobustKernel):\n",
    "    \"\"\"Cauchy (Lorentzian) robust kernel\"\"\"\n",
    "    def weight(self, error: float) -> float:\n",
    "        return 1.0 / (1.0 + (error / self.delta)**2)\n",
    "    \n",
    "    def loss(self, error: float) -> float:\n",
    "        return 0.5 * self.delta**2 * np.log(1.0 + (error / self.delta)**2)\n",
    "\n",
    "class TukeyKernel(RobustKernel):\n",
    "    \"\"\"Tukey biweight robust kernel\"\"\"\n",
    "    def weight(self, error: float) -> float:\n",
    "        abs_error = abs(error)\n",
    "        if abs_error <= self.delta:\n",
    "            return (1.0 - (error / self.delta)**2)**2\n",
    "        else:\n",
    "            return 0.0\n",
    "    \n",
    "    def loss(self, error: float) -> float:\n",
    "        abs_error = abs(error)\n",
    "        if abs_error <= self.delta:\n",
    "            r = error / self.delta\n",
    "            return (self.delta**2 / 6.0) * (1.0 - (1.0 - r**2)**3)\n",
    "        else:\n",
    "            return self.delta**2 / 6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizing Robust Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize different robust kernels\n",
    "errors = np.linspace(-5, 5, 1000)\n",
    "delta = 1.0\n",
    "\n",
    "kernels = {\n",
    "    'L2': L2Kernel(delta),\n",
    "    'Huber': HuberKernel(delta),\n",
    "    'Cauchy': CauchyKernel(delta),\n",
    "    'Tukey': TukeyKernel(delta)\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Plot loss functions\n",
    "ax = axes[0]\n",
    "for name, kernel in kernels.items():\n",
    "    losses = [kernel.loss(e) for e in errors]\n",
    "    ax.plot(errors, losses, label=name, linewidth=2)\n",
    "ax.set_xlabel('Error')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Robust Loss Functions')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim([0, 5])\n",
    "\n",
    "# Plot weights\n",
    "ax = axes[1]\n",
    "for name, kernel in kernels.items():\n",
    "    weights = [kernel.weight(e) for e in errors]\n",
    "    ax.plot(errors, weights, label=name, linewidth=2)\n",
    "ax.set_xlabel('Error')\n",
    "ax.set_ylabel('Weight')\n",
    "ax.set_title('Weights for Weighted Least Squares')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot influence functions (derivative of loss)\n",
    "ax = axes[2]\n",
    "for name, kernel in kernels.items():\n",
    "    influences = []\n",
    "    for e in errors:\n",
    "        if name == 'L2':\n",
    "            influences.append(e)\n",
    "        else:\n",
    "            influences.append(e * kernel.weight(e))\n",
    "    ax.plot(errors, influences, label=name, linewidth=2)\n",
    "ax.set_xlabel('Error')\n",
    "ax.set_ylabel('Influence')\n",
    "ax.set_title('Influence Functions')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Robust Pose Graph Optimizer\n",
    "\n",
    "Now let's implement a pose graph optimizer with robust kernels using Iteratively Reweighted Least Squares (IRLS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustPoseGraphOptimizer:\n",
    "    def __init__(self, kernel: RobustKernel = None):\n",
    "        self.vertices = {}  # id -> (x, y, theta)\n",
    "        self.edges = []     # (i, j, dx, dy, dtheta, info_matrix)\n",
    "        self.kernel = kernel if kernel else L2Kernel()\n",
    "        self.fixed_vertices = set()\n",
    "    \n",
    "    def add_vertex(self, vertex_id: int, x: float, y: float, theta: float):\n",
    "        self.vertices[vertex_id] = np.array([x, y, theta])\n",
    "    \n",
    "    def add_edge(self, i: int, j: int, dx: float, dy: float, dtheta: float, \n",
    "                 info_matrix: np.ndarray = None):\n",
    "        if info_matrix is None:\n",
    "            info_matrix = np.eye(3)\n",
    "        self.edges.append((i, j, np.array([dx, dy, dtheta]), info_matrix))\n",
    "    \n",
    "    def fix_vertex(self, vertex_id: int):\n",
    "        self.fixed_vertices.add(vertex_id)\n",
    "    \n",
    "    def _normalize_angle(self, angle: float) -> float:\n",
    "        return np.arctan2(np.sin(angle), np.cos(angle))\n",
    "    \n",
    "    def _compute_error(self, i: int, j: int, measurement: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute error between predicted and measured relative pose\"\"\"\n",
    "        xi = self.vertices[i]\n",
    "        xj = self.vertices[j]\n",
    "        \n",
    "        # Predicted relative pose\n",
    "        c = np.cos(xi[2])\n",
    "        s = np.sin(xi[2])\n",
    "        dx_pred = c * (xj[0] - xi[0]) + s * (xj[1] - xi[1])\n",
    "        dy_pred = -s * (xj[0] - xi[0]) + c * (xj[1] - xi[1])\n",
    "        dtheta_pred = self._normalize_angle(xj[2] - xi[2])\n",
    "        \n",
    "        # Error\n",
    "        error = np.array([\n",
    "            dx_pred - measurement[0],\n",
    "            dy_pred - measurement[1],\n",
    "            self._normalize_angle(dtheta_pred - measurement[2])\n",
    "        ])\n",
    "        \n",
    "        return error\n",
    "    \n",
    "    def _compute_jacobians(self, i: int, j: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Compute Jacobians of error with respect to poses i and j\"\"\"\n",
    "        xi = self.vertices[i]\n",
    "        xj = self.vertices[j]\n",
    "        \n",
    "        c = np.cos(xi[2])\n",
    "        s = np.sin(xi[2])\n",
    "        dx = xj[0] - xi[0]\n",
    "        dy = xj[1] - xi[1]\n",
    "        \n",
    "        # Jacobian with respect to pose i\n",
    "        Ji = np.array([\n",
    "            [-c, -s, -s * dx + c * dy],\n",
    "            [s, -c, -c * dx - s * dy],\n",
    "            [0, 0, -1]\n",
    "        ])\n",
    "        \n",
    "        # Jacobian with respect to pose j\n",
    "        Jj = np.array([\n",
    "            [c, s, 0],\n",
    "            [-s, c, 0],\n",
    "            [0, 0, 1]\n",
    "        ])\n",
    "        \n",
    "        return Ji, Jj\n",
    "    \n",
    "    def optimize(self, max_iterations: int = 20, tolerance: float = 1e-6, \n",
    "                 verbose: bool = True) -> Dict[str, List[float]]:\n",
    "        \"\"\"Optimize using Iteratively Reweighted Least Squares (IRLS)\"\"\"\n",
    "        n_vertices = len(self.vertices)\n",
    "        vertex_ids = sorted(self.vertices.keys())\n",
    "        id_to_idx = {v_id: idx for idx, v_id in enumerate(vertex_ids)}\n",
    "        \n",
    "        history = {\n",
    "            'total_error': [],\n",
    "            'max_update': [],\n",
    "            'iteration_time': []\n",
    "        }\n",
    "        \n",
    "        for iteration in range(max_iterations):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Build system matrix\n",
    "            H = np.zeros((n_vertices * 3, n_vertices * 3))\n",
    "            b = np.zeros(n_vertices * 3)\n",
    "            \n",
    "            total_error = 0.0\n",
    "            \n",
    "            for i, j, measurement, info_matrix in self.edges:\n",
    "                # Compute error and Jacobians\n",
    "                error = self._compute_error(i, j, measurement)\n",
    "                Ji, Jj = self._compute_jacobians(i, j)\n",
    "                \n",
    "                # Compute robust weight\n",
    "                error_norm = np.sqrt(error.T @ info_matrix @ error)\n",
    "                weight = self.kernel.weight(error_norm)\n",
    "                \n",
    "                # Apply weight to information matrix\n",
    "                weighted_info = weight * info_matrix\n",
    "                \n",
    "                # Add contribution to H and b\n",
    "                idx_i = id_to_idx[i] * 3\n",
    "                idx_j = id_to_idx[j] * 3\n",
    "                \n",
    "                # H_ii += Ji^T * Omega * Ji\n",
    "                H[idx_i:idx_i+3, idx_i:idx_i+3] += Ji.T @ weighted_info @ Ji\n",
    "                # H_ij += Ji^T * Omega * Jj\n",
    "                H[idx_i:idx_i+3, idx_j:idx_j+3] += Ji.T @ weighted_info @ Jj\n",
    "                # H_ji += Jj^T * Omega * Ji\n",
    "                H[idx_j:idx_j+3, idx_i:idx_i+3] += Jj.T @ weighted_info @ Ji\n",
    "                # H_jj += Jj^T * Omega * Jj\n",
    "                H[idx_j:idx_j+3, idx_j:idx_j+3] += Jj.T @ weighted_info @ Jj\n",
    "                \n",
    "                # b_i += -Ji^T * Omega * error\n",
    "                b[idx_i:idx_i+3] += -Ji.T @ weighted_info @ error\n",
    "                # b_j += -Jj^T * Omega * error\n",
    "                b[idx_j:idx_j+3] += -Jj.T @ weighted_info @ error\n",
    "                \n",
    "                # Accumulate total error (using robust loss)\n",
    "                total_error += self.kernel.loss(error_norm)\n",
    "            \n",
    "            # Fix first vertex\n",
    "            for fixed_id in self.fixed_vertices:\n",
    "                idx = id_to_idx[fixed_id] * 3\n",
    "                H[idx:idx+3, :] = 0\n",
    "                H[:, idx:idx+3] = 0\n",
    "                H[idx:idx+3, idx:idx+3] = np.eye(3)\n",
    "                b[idx:idx+3] = 0\n",
    "            \n",
    "            # Solve H * dx = b\n",
    "            try:\n",
    "                dx = np.linalg.solve(H, b)\n",
    "            except np.linalg.LinAlgError:\n",
    "                print(f\"Warning: Singular matrix at iteration {iteration}\")\n",
    "                break\n",
    "            \n",
    "            # Update poses\n",
    "            for v_id in vertex_ids:\n",
    "                idx = id_to_idx[v_id] * 3\n",
    "                self.vertices[v_id] += dx[idx:idx+3]\n",
    "                # Normalize angle\n",
    "                self.vertices[v_id][2] = self._normalize_angle(self.vertices[v_id][2])\n",
    "            \n",
    "            # Record history\n",
    "            max_update = np.max(np.abs(dx))\n",
    "            iteration_time = time.time() - start_time\n",
    "            \n",
    "            history['total_error'].append(total_error)\n",
    "            history['max_update'].append(max_update)\n",
    "            history['iteration_time'].append(iteration_time)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Iteration {iteration}: error = {total_error:.6f}, \"\n",
    "                      f\"max update = {max_update:.6f}, time = {iteration_time:.3f}s\")\n",
    "            \n",
    "            # Check convergence\n",
    "            if max_update < tolerance:\n",
    "                if verbose:\n",
    "                    print(f\"Converged after {iteration + 1} iterations\")\n",
    "                break\n",
    "        \n",
    "        return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Testing with Outliers\n",
    "\n",
    "Let's create a test scenario with outliers to demonstrate the effectiveness of robust optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_graph_with_outliers(n_poses: int = 20, outlier_ratio: float = 0.1):\n",
    "    \"\"\"Create a circular trajectory with some outlier measurements\"\"\"\n",
    "    optimizer = RobustPoseGraphOptimizer()\n",
    "    \n",
    "    # Ground truth poses (circle)\n",
    "    radius = 10.0\n",
    "    angles = np.linspace(0, 2*np.pi, n_poses, endpoint=False)\n",
    "    ground_truth = []\n",
    "    \n",
    "    for i, angle in enumerate(angles):\n",
    "        x = radius * np.cos(angle)\n",
    "        y = radius * np.sin(angle)\n",
    "        theta = angle + np.pi/2  # Tangent direction\n",
    "        ground_truth.append([x, y, theta])\n",
    "        \n",
    "        # Add noisy initial guess\n",
    "        x_init = x + np.random.normal(0, 0.5)\n",
    "        y_init = y + np.random.normal(0, 0.5)\n",
    "        theta_init = theta + np.random.normal(0, 0.1)\n",
    "        optimizer.add_vertex(i, x_init, y_init, theta_init)\n",
    "    \n",
    "    # Fix first vertex\n",
    "    optimizer.fix_vertex(0)\n",
    "    \n",
    "    # Add sequential edges\n",
    "    for i in range(n_poses):\n",
    "        j = (i + 1) % n_poses\n",
    "        \n",
    "        # Compute true relative pose\n",
    "        dx_true = ground_truth[j][0] - ground_truth[i][0]\n",
    "        dy_true = ground_truth[j][1] - ground_truth[i][1]\n",
    "        c = np.cos(ground_truth[i][2])\n",
    "        s = np.sin(ground_truth[i][2])\n",
    "        dx_local = c * dx_true + s * dy_true\n",
    "        dy_local = -s * dx_true + c * dy_true\n",
    "        dtheta = ground_truth[j][2] - ground_truth[i][2]\n",
    "        \n",
    "        # Add noise\n",
    "        dx_meas = dx_local + np.random.normal(0, 0.1)\n",
    "        dy_meas = dy_local + np.random.normal(0, 0.1)\n",
    "        dtheta_meas = dtheta + np.random.normal(0, 0.05)\n",
    "        \n",
    "        # Randomly make some measurements outliers\n",
    "        if np.random.random() < outlier_ratio:\n",
    "            # Add large error to create outlier\n",
    "            dx_meas += np.random.normal(0, 2.0)\n",
    "            dy_meas += np.random.normal(0, 2.0)\n",
    "            dtheta_meas += np.random.normal(0, 0.5)\n",
    "        \n",
    "        optimizer.add_edge(i, j, dx_meas, dy_meas, dtheta_meas)\n",
    "    \n",
    "    # Add loop closure edges with higher chance of outliers\n",
    "    n_loop_closures = 5\n",
    "    for _ in range(n_loop_closures):\n",
    "        i = np.random.randint(0, n_poses - 5)\n",
    "        j = i + np.random.randint(5, min(n_poses - i, 10))\n",
    "        \n",
    "        # Compute relative pose\n",
    "        dx_true = ground_truth[j][0] - ground_truth[i][0]\n",
    "        dy_true = ground_truth[j][1] - ground_truth[i][1]\n",
    "        c = np.cos(ground_truth[i][2])\n",
    "        s = np.sin(ground_truth[i][2])\n",
    "        dx_local = c * dx_true + s * dy_true\n",
    "        dy_local = -s * dx_true + c * dy_true\n",
    "        dtheta = ground_truth[j][2] - ground_truth[i][2]\n",
    "        \n",
    "        # Add noise (higher chance of outliers for loop closures)\n",
    "        dx_meas = dx_local + np.random.normal(0, 0.2)\n",
    "        dy_meas = dy_local + np.random.normal(0, 0.2)\n",
    "        dtheta_meas = dtheta + np.random.normal(0, 0.1)\n",
    "        \n",
    "        if np.random.random() < 0.3:  # 30% outlier rate for loop closures\n",
    "            dx_meas += np.random.normal(0, 3.0)\n",
    "            dy_meas += np.random.normal(0, 3.0)\n",
    "            dtheta_meas += np.random.normal(0, 1.0)\n",
    "        \n",
    "        optimizer.add_edge(i, j, dx_meas, dy_meas, dtheta_meas)\n",
    "    \n",
    "    return optimizer, np.array(ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparing Different Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test scenario\n",
    "np.random.seed(42)\n",
    "n_poses = 30\n",
    "outlier_ratio = 0.15\n",
    "\n",
    "# Test different kernels\n",
    "kernels_to_test = {\n",
    "    'L2 (No Robustness)': L2Kernel(),\n",
    "    'Huber': HuberKernel(delta=1.0),\n",
    "    'Cauchy': CauchyKernel(delta=1.0),\n",
    "    'Tukey': TukeyKernel(delta=2.0)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "optimizers = {}\n",
    "\n",
    "for kernel_name, kernel in kernels_to_test.items():\n",
    "    print(f\"\\nOptimizing with {kernel_name} kernel...\")\n",
    "    \n",
    "    # Create fresh optimizer with same initial conditions\n",
    "    optimizer, ground_truth = create_test_graph_with_outliers(n_poses, outlier_ratio)\n",
    "    optimizer.kernel = kernel\n",
    "    \n",
    "    # Store initial poses for visualization\n",
    "    initial_poses = {k: v.copy() for k, v in optimizer.vertices.items()}\n",
    "    \n",
    "    # Optimize\n",
    "    history = optimizer.optimize(max_iterations=50, verbose=False)\n",
    "    \n",
    "    results[kernel_name] = history\n",
    "    optimizers[kernel_name] = optimizer\n",
    "    \n",
    "    print(f\"Final error: {history['total_error'][-1]:.6f}\")\n",
    "    print(f\"Iterations: {len(history['total_error'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize convergence\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot error convergence\n",
    "ax = axes[0, 0]\n",
    "for kernel_name, history in results.items():\n",
    "    ax.semilogy(history['total_error'], label=kernel_name, linewidth=2)\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Total Error (log scale)')\n",
    "ax.set_title('Error Convergence')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot max update convergence\n",
    "ax = axes[0, 1]\n",
    "for kernel_name, history in results.items():\n",
    "    ax.semilogy(history['max_update'], label=kernel_name, linewidth=2)\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Max Update (log scale)')\n",
    "ax.set_title('Update Magnitude Convergence')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot final trajectories\n",
    "for idx, (kernel_name, optimizer) in enumerate(optimizers.items()):\n",
    "    if idx < 2:\n",
    "        ax = axes[1, idx]\n",
    "        \n",
    "        # Plot ground truth\n",
    "        gt_x = [p[0] for p in ground_truth]\n",
    "        gt_y = [p[1] for p in ground_truth]\n",
    "        ax.plot(gt_x + [gt_x[0]], gt_y + [gt_y[0]], 'g-', linewidth=2, \n",
    "                label='Ground Truth', alpha=0.7)\n",
    "        \n",
    "        # Plot optimized trajectory\n",
    "        opt_poses = [optimizer.vertices[i] for i in sorted(optimizer.vertices.keys())]\n",
    "        opt_x = [p[0] for p in opt_poses]\n",
    "        opt_y = [p[1] for p in opt_poses]\n",
    "        ax.plot(opt_x + [opt_x[0]], opt_y + [opt_y[0]], 'b-', linewidth=2, \n",
    "                label='Optimized', marker='o', markersize=3)\n",
    "        \n",
    "        # Plot edges (to show outliers)\n",
    "        for i, j, _, _ in optimizer.edges[:n_poses]:  # Only sequential edges\n",
    "            pi = optimizer.vertices[i]\n",
    "            pj = optimizer.vertices[j]\n",
    "            ax.plot([pi[0], pj[0]], [pi[1], pj[1]], 'r-', alpha=0.2, linewidth=0.5)\n",
    "        \n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_xlabel('X')\n",
    "        ax.set_ylabel('Y')\n",
    "        ax.set_title(f'Result: {kernel_name}')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyzing Edge Weights\n",
    "\n",
    "Let's visualize which edges are identified as outliers by different robust kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute final weights for each edge with different kernels\n",
    "edge_weights = {kernel_name: [] for kernel_name in kernels_to_test.keys()}\n",
    "\n",
    "for kernel_name, optimizer in optimizers.items():\n",
    "    weights = []\n",
    "    errors = []\n",
    "    \n",
    "    for i, j, measurement, info_matrix in optimizer.edges:\n",
    "        error = optimizer._compute_error(i, j, measurement)\n",
    "        error_norm = np.sqrt(error.T @ info_matrix @ error)\n",
    "        weight = optimizer.kernel.weight(error_norm)\n",
    "        weights.append(weight)\n",
    "        errors.append(error_norm)\n",
    "    \n",
    "    edge_weights[kernel_name] = (weights, errors)\n",
    "\n",
    "# Visualize edge weights\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (kernel_name, (weights, errors)) in enumerate(edge_weights.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Create scatter plot colored by weight\n",
    "    edge_indices = np.arange(len(weights))\n",
    "    scatter = ax.scatter(edge_indices, errors, c=weights, \n",
    "                        cmap='RdYlGn', vmin=0, vmax=1, s=50)\n",
    "    \n",
    "    ax.set_xlabel('Edge Index')\n",
    "    ax.set_ylabel('Error Magnitude')\n",
    "    ax.set_title(f'{kernel_name}: Edge Errors and Weights')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=ax)\n",
    "    cbar.set_label('Weight')\n",
    "    \n",
    "    # Mark outliers (low weight edges)\n",
    "    outlier_threshold = 0.3\n",
    "    outlier_indices = [i for i, w in enumerate(weights) if w < outlier_threshold]\n",
    "    if outlier_indices:\n",
    "        ax.scatter(outlier_indices, [errors[i] for i in outlier_indices],\n",
    "                  marker='x', s=100, c='red', linewidth=2, \n",
    "                  label=f'Outliers (w < {outlier_threshold})')\n",
    "        ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Comparison\n",
    "\n",
    "Let's compare the final trajectory errors for each kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute trajectory errors\n",
    "trajectory_errors = {}\n",
    "\n",
    "for kernel_name, optimizer in optimizers.items():\n",
    "    errors = []\n",
    "    for i in range(len(ground_truth)):\n",
    "        gt = ground_truth[i]\n",
    "        est = optimizer.vertices[i]\n",
    "        \n",
    "        # Position error\n",
    "        pos_error = np.linalg.norm(gt[:2] - est[:2])\n",
    "        \n",
    "        # Orientation error\n",
    "        angle_error = abs(optimizer._normalize_angle(gt[2] - est[2]))\n",
    "        \n",
    "        errors.append((pos_error, angle_error))\n",
    "    \n",
    "    trajectory_errors[kernel_name] = errors\n",
    "\n",
    "# Create comparison table\n",
    "print(\"\\nTrajectory Error Statistics:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Kernel':<20} {'Mean Pos Error':<15} {'Max Pos Error':<15} \"\n",
    "      f\"{'Mean Angle Error':<18} {'Max Angle Error':<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for kernel_name, errors in trajectory_errors.items():\n",
    "    pos_errors = [e[0] for e in errors]\n",
    "    angle_errors = [e[1] for e in errors]\n",
    "    \n",
    "    print(f\"{kernel_name:<20} {np.mean(pos_errors):<15.4f} {np.max(pos_errors):<15.4f} \"\n",
    "          f\"{np.mean(angle_errors):<18.4f} {np.max(angle_errors):<15.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Symforce Implementation with Robust Kernels\n",
    "\n",
    "Let's implement a robust kernel using Symforce for automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cauchy_robust_residual(\n",
    "    pose_i: sf.Pose2,\n",
    "    pose_j: sf.Pose2,\n",
    "    measurement: sf.Pose2,\n",
    "    delta: sf.Scalar,\n",
    "    epsilon: sf.Scalar\n",
    ") -> sf.V3:\n",
    "    \"\"\"\n",
    "    Compute robust residual using Cauchy kernel\n",
    "    \n",
    "    The Cauchy kernel is applied to the error norm:\n",
    "    rho(e) = delta^2 * log(1 + (e/delta)^2)\n",
    "    \n",
    "    The residual is then: sqrt(2 * rho(||e||)) * e / ||e||\n",
    "    \"\"\"\n",
    "    # Compute relative pose error\n",
    "    T_ij_predicted = pose_i.inverse() * pose_j\n",
    "    T_error = measurement.inverse() * T_ij_predicted\n",
    "    \n",
    "    # Extract error components\n",
    "    error = sf.V3(\n",
    "        T_error.position()[0],\n",
    "        T_error.position()[1],\n",
    "        T_error.rotation().to_tangent()[0]\n",
    "    )\n",
    "    \n",
    "    # Compute error norm\n",
    "    error_norm = error.norm(epsilon=epsilon)\n",
    "    \n",
    "    # Apply Cauchy robust function\n",
    "    # rho(e) = delta^2 * log(1 + (e/delta)^2)\n",
    "    rho = delta**2 * sf.log(1 + (error_norm / delta)**2)\n",
    "    \n",
    "    # Compute sqrt(2 * rho) which gives the robust residual magnitude\n",
    "    robust_magnitude = sf.sqrt(2 * rho + epsilon)\n",
    "    \n",
    "    # Scale error to get robust residual\n",
    "    # This ensures the Jacobian is correct\n",
    "    robust_residual = (robust_magnitude / (error_norm + epsilon)) * error\n",
    "    \n",
    "    return robust_residual\n",
    "\n",
    "# Test the Symforce robust kernel\n",
    "from symforce.codegen import Codegen, CppConfig\n",
    "\n",
    "# Generate optimized code\n",
    "codegen = Codegen.function(\n",
    "    func=cauchy_robust_residual,\n",
    "    config=CppConfig()\n",
    ")\n",
    "\n",
    "# Generate Python function\n",
    "cauchy_residual_py = codegen.with_jacobians(\n",
    "    which_args=[\"pose_i\", \"pose_j\"],\n",
    "    include_results=True\n",
    ").generate_function()\n",
    "\n",
    "print(\"Generated robust residual function with automatic Jacobians!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exercise: Adaptive Robust Kernels\n",
    "\n",
    "Try implementing an adaptive robust kernel that adjusts its threshold based on the distribution of errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveCauchyKernel(RobustKernel):\n",
    "    \"\"\"Cauchy kernel with adaptive threshold based on error distribution\"\"\"\n",
    "    def __init__(self, percentile: float = 0.7):\n",
    "        super().__init__(delta=1.0)\n",
    "        self.percentile = percentile\n",
    "        self.error_history = []\n",
    "    \n",
    "    def update_threshold(self, errors: List[float]):\n",
    "        \"\"\"Update delta based on error distribution\"\"\"\n",
    "        if len(errors) > 10:\n",
    "            # Use median absolute deviation (MAD) for robust scale estimate\n",
    "            median_error = np.median(errors)\n",
    "            mad = np.median(np.abs(errors - median_error))\n",
    "            # Scale factor for Cauchy distribution\n",
    "            self.delta = 1.4826 * mad * self.percentile\n",
    "    \n",
    "    def weight(self, error: float) -> float:\n",
    "        self.error_history.append(abs(error))\n",
    "        return super().weight(error)\n",
    "\n",
    "# TODO: Implement and test the adaptive kernel\n",
    "# 1. Modify the optimizer to use the adaptive kernel\n",
    "# 2. Update the threshold after each iteration\n",
    "# 3. Compare with fixed-threshold kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this chapter, we learned:\n",
    "\n",
    "1. **Why robust optimization is crucial** for handling outliers in SLAM\n",
    "2. **Different robust kernels** (Huber, Cauchy, Tukey) and their properties\n",
    "3. **Iteratively Reweighted Least Squares (IRLS)** for robust optimization\n",
    "4. How to **implement robust pose graph optimization**\n",
    "5. The **dramatic improvement** robust kernels provide with outliers\n",
    "6. How to use **Symforce for automatic differentiation** of robust residuals\n",
    "\n",
    "Key takeaways:\n",
    "- L2 optimization fails catastrophically with outliers\n",
    "- Cauchy and Tukey kernels effectively down-weight outliers\n",
    "- IRLS provides an elegant way to solve robust optimization problems\n",
    "- Proper threshold selection is important for robust kernel performance\n",
    "\n",
    "Next chapter: We'll explore rotation initialization using chordal relaxation!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
